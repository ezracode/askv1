{
    "Recursive": {
        "avg_faithfulness": 0.8333333333333334,
        "avg_context_relevance": 0.8333333333333334,
        "avg_answer_relevance": 1.0,
        "details": [
            {
                "question": "¿Cuál es el uso más común que se le da a la técnica Pose Estimation?",
                "answer": " El uso más común de la técnica Pose Estimation es en la aplicación de realidad aumentada (AR). En esta aplicación, imágenes o anotaciones virtuales se superponen sobre un vídeo en vivo, ya sea a través de gafas de realidad aumentada (un display en cabeza) o en una pantalla de computadora o dispositivo móvil. En algunos casos, se utiliza un patrón especialmente impreso en tarjetas o en un libro para realizar la augmentación. Para una aplicación de escritorio, una red de puntos impresos en un ratón puede ser rastreada por una cámara incorporada en un ratón de realidad aumentada para dar al usuario control de su posición y orientación en un espacio tridimensional (Hinckley, Sinclair et al. 1999).\n\nFuentes: Azuma, Baillot et al. (2001); Haller, Billinghurst, and Thomas (2007); Billinghurst, Clark, and Lee (2015); Kato, Billinghurst et al. (2000); Billinghurst, Kato, and Poupyrev (2001); Hinckley, Sinclair et al. (1999).",
                "context": "11.2 Pose estimation 693\n11.2 Pose estimation\nA particular instance of feature-based alignment, which occurs very often, is estimating an\nobject’s 3D pose from a set of 2D point projections. This pose estimation problem is also\nknown as extrinsic calibration, as opposed to the intrinsic calibration of internal camera pa-\nrameters such as focal length, which we discuss in Section 11.1. The problem of recovering\npose from three correspondences, which is the minimal amount of information necessary,\nis known as the perspective-3-point-problem (P3P),2 with extensions to larger numbers of\npoints collectively known as PnP (Haralick, Lee et al. 1994; Quan and Lan 1999; Gao, Hou\net al. 2003; Moreno-Noguer, Lepetit, and Fua 2007; Persson and Nordberg 2018).\nIn this section, we look at some of the techniques that have been developed to solve such\nproblems, starting with thedirect linear transform(DLT), which recovers a 3×4 camera ma-\ntrix, followed by other “linear” algorithms, and then looking at statistically optimal iterative\nalgorithms.\n11.2.1 Linear algorithms\nThe simplest way to recover the pose of the camera is to form a set of rational linear equations\nanalogous to those used for 2D motion estimation (8.19) from the camera matrix form of\nperspective projection (2.55–2.56),\nxi = p00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(11.11)\nyi = p10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n, (11.12)\n\n---\n\nrelated work, Saxena, Sun, and Ng (2009) develop a system that directly infers the depth and\norientation of each pixel instead of using just three geometric class labels. We will examine\ntechniques to infer depth from single images in more detail in Section 12.8.\n6.4.5 Pose estimation\nThe inference of human pose (head, body, and limb locations and attitude) from a single\nimages can be viewed as yet another kind of segmentation task. We have already discussed\nsome pose estimation techniques in Section 6.3.2 on pedestrian detection section, as shown\nin Figure 6.25. Starting with the seminal work by Felzenszwalb and Huttenlocher (2005),\n2D and 3D pose detection and estimation rapidly developed as an active research area, with\nimportant advances and datasets (Sigal and Black 2006a; Rogez, Rihanet al. 2008; Andriluka,\nRoth, and Schiele 2009; Bourdev and Malik 2009; Johnson and Everingham 2011; Yang\nand Ramanan 2011; Pishchulin, Andriluka et al. 2013; Sapp and Taskar 2013; Andriluka,\nPishchulin et al. 2014).\nMore recently, deep networks have become the preferred technique to identify human\nbody keypoints in order to convert these into pose estimates (Tompson, Jain et al. 2014;\n\n---\n\nWhile the preceding body ﬁtting and pose estimation systems use multiple views to es-\ntimate body shape, Guan, Weiss et al. (2009) ﬁt a human shape and pose model to a single\nimage of a person on a natural background. Manual initialization is used to estimate a rough\npose (skeleton) and height model, and this is then used to segment the person’s outline using\nthe Grab Cut segmentation algorithm (Section 4.3.2). The shape and pose estimate are then\nreﬁned using a combination of silhouette edge cues and shading information (Figure 13.29).\nThe resulting 3D model can be used to create novel animations.\n\n---\n\n3D vector, as explained in Equation (2.36).\nTarget-based augmented reality\nA widely used application of pose estimation is augmented reality, where virtual 3D images\nor annotations are superimposed on top of a live video feed, either through the use of see-\nthrough glasses (a head-mounted display) or on a regular computer or mobile device screen\n(Azuma, Baillot et al. 2001; Haller, Billinghurst, and Thomas 2007; Billinghurst, Clark, and\nLee 2015). In some applications, a special pattern printed on cards or in a book is tracked to\nperform the augmentation (Kato, Billinghurst et al. 2000; Billinghurst, Kato, and Poupyrev\n2001). For a desktop application, a grid of dots printed on a mouse pad can be tracked by\na camera embedded in an augmented mouse to give the user control of a full six degrees of\nfreedom over their position and orientation in a 3D space (Hinckley, Sinclair et al. 1999).\nToday, tracking known targets such as movie posters is used in some phone-based augmented\nreality systems such as Facebook’s Spark AR.5\nSometimes, the scene itself provides a convenient object to track, such as the rectangle\ndeﬁning a desktop used in through-the-lens camera control (Gleicher and Witkin 1992). In\noutdoor locations, such as ﬁlm sets, it is more common to place special markers such as\nbrightly colored balls in the scene to make it easier to ﬁnd and track them (Bogart 1991). In\nolder applications, surveying techniques were used to determine the locations of these balls\n\n---\n\n11.2 Pose estimation 695\npj (Figure 11.6).\nA full derivation of this approach can be found in the ﬁrst edition of this book (Szeliski\n2010, Section 6.2.1) and also in (Quan and Lan 1999), where the authors provide accuracy\nresults for this and other techniques, which use fewer points but require more complicated\nalgebraic manipulations. The paper by Moreno-Noguer, Lepetit, and Fua (2007) reviews\nother alternatives and also gives a lower complexity algorithm that typically produces more\naccurate results. An even more recent paper by Terzakis and Lourakis (2020) reviews papers\npublished in the last decade.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 11.4.5), it is prudent to optimize\nthe initial estimates from PnP using the iterative technique described in Section 11.2.2. An\nalternative pose estimation algorithm involves starting with a scaled orthographic projection\nmodel and then iteratively reﬁning this initial estimate using a more accurate perspective\nprojection model (DeMenthon and Davis 1995). The attraction of this model, as stated in the\npaper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\nCNN-based pose estimation\nAs with other areas on computer vision, deep neural networks have also been applied to pose\nestimation. Some representative papers include Xiang, Schmidt et al. (2018), Oberweger,\n\n---\n\na camera’s intrinsic parameters. In photogrammetry, it is common to set up a camera in a\nlarge ﬁeld looking at distant calibration targets whose exact location has been precomputed\nusing surveying equipment (Slama 1980; Atkinson 1996; Kraus 1997). In this case, the trans-\nlational component of the pose becomes irrelevant and only the camera rotation and intrinsic\nparameters need to be recovered.\nIf a smaller calibration rig needs to be used, e.g., for indoor robotics applications or for\nmobile robots that carry their own calibration target, it is best if the calibration object can span\nas much of the workspace as possible (Figure 11.2a), as planar targets often fail to accurately\npredict the components of the pose that lie far away from the plane. A good way to determine\nif the calibration has been successfully performed is to estimate the covariance in the param-\neters (Section 8.1.4) and then project 3D points from various points in the workspace into the\nimage in order to estimate their 2D positional uncertainty.\nIf no calibration pattern is available, it is also possible to perform calibration simulta-\nneously with structure and pose recovery (Sections 11.1.3 and 11.4.2), which is known as\nself-calibration (Faugeras, Luong, and Maybank 1992; Pollefeys, Koch, and Van Gool 1999;\nHartley and Zisserman 2004; Moons, Van Gool, and Vergauwen 2010). However, such an\napproach requires a large amount of imagery to be accurate.",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿En que casos es útil la programación dinámica?",
                "answer": " La programación dinámica es útil en el procesamiento de imágenes y la segmentación estéreo, donde se necesita encontrar la mejor asignación entre píxeles de dos imágenes diferentes. Sin embargo, tiene algunos problemas como la selección del costo para los píxeles ocultos, la dificultad de garantizar la consistencia entre líneas de escaneo y el requisito de mantener la ordenación o monotonía de los píxeles en las dos vistas. Una alternativa a la programación dinámica es negociar las restricciones de suavidad vertical en (12.9) y simplemente optimizar líneas de escaneo independientes en la función energética global (12.7). Esta aproximación se utiliza para obtener resultados más rápidos, pero aún puede presentar artefactos de rayado.",
                "context": "views, which may not be the case in scenes containing narrow foreground objects.\nAn alternative to traditional dynamic programming, introduced by Scharstein and Szeliski\n(2002), is to neglect the vertical smoothness constraints in (12.9) and simply optimize inde-\npendent scanlines in the global energy function (12.7). The advantage of this scanline op-\ntimization algorithm is that it computes the same representation and minimizes a reduced\nversion of the same energy function as the full 2D energy function (12.7). Unfortunately, it\nstill suffers from the same streaking artifacts as dynamic programming. Dynamic program-\nming is also possible on tree structures, which can ameliorate the streaking (Veksler 2005).\nMuch higher quality results can be obtained by summing up the cumulative cost function\nfrom multiple directions, e.g, from the eight cardinal directions, N, E, W, S, NE, SE, SW,\nNW (Hirschm ¨uller 2008). The resulting semi-global matching (SGM) algorithm performs\nquite well and is extremely efﬁcient, enabling real-time low-power implementations (Gehrig,\nEberli, and Meyer 2009). Drory, Haubold et al. (2014) show that SGM is equivalent to early\nstopping for a particular variant of belief propagation. Semi-global matching has also been\nextended using learned components, e.g., SGM-Net (Seki and Pollefeys 2017), which uses a\nCNN to adjust transition costs, and SGM-Forest (Sch ¨onberger, Sinha, and Pollefeys 2018),\n\n---\n\n24 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nand still others as open-ended research problems that make for challenging ﬁnal projects.\nMotivated students who implement a reasonable subset of these exercises will, by the end of\nthe book, have a computer vision software library that can be used for a variety of interesting\ntasks and projects.\nIf the students or curriculum do not have a strong preference for programming languages,\nPython, with the NumPy scientiﬁc and array arithmetic library plus the OpenCV vision li-\nbrary, are a good environment to develop algorithms and learn about vision. Not only will the\nstudents learn how to program using array/tensor notation and linear/matrix algebra (which is\na good foundation for later use of PyTorch for deep learning), you can also prepare classroom\nassignments using Jupyter notebooks, giving you the option to combine descriptive tutorials,\nsample code, and code to be extended/modiﬁed in one convenient location.10\nAs this is a reference book, I try wherever possible to discuss which techniques and al-\ngorithms work well in practice, as well as provide up-to-date pointers to the latest research\nresults in the areas that I cover. The exercises can be used to build up your own personal\nlibrary of self-tested and validated vision algorithms, which is more worthwhile in the long\nterm (assuming you have the time) than simply pulling algorithms out of a library whose\n\n---\n\narray-based (matrix) arithmetic and (hopefully) dissuades them from writing pixel-iteration\nloops that are slow to write and error-prone. A big advantage of writing in this fashion is that\nit maps closely to the abstractions used in the deep learning frameworks such as PyTorch and\nTensorFlow. It also often results in highly optimized code that can be run on both CPUs and\nGPUs with minimal changes.3\nIn the rest of this section, I list some additional software packages and libraries that stu-\ndents may ﬁnd useful. You can also ﬁnd pointers to older (currently less used) software\npackages in the ﬁrst edition of this book (Szeliski 2010, Appendix C.2).\nChapter 3: Image processing\n• Before diving into OpenCV , I would encourage you to write some simple image pro-\ncessing functions in NumPy using the built-in multidimensional array notation. It’s\nﬁne to use OpenCV for image input/output and to use Matplotlib for visualization.\nThere are also other high-level packages for image processing, such as scikit-image\nand PIL/Pillow. A more recently developed computer vision library is MMCV (https:\n//openmmlab.com/codebase#MMCV).\n• As a warm-up exercise, before diving into machine learning but after doing the ba-\nsic PyTorch or TensorFlow tutorials, try porting your NumPy code into one of these\nlanguages.\n• Another language that supports array-level functional programming is Halide (https:\n//halide-lang.org) (Ragan-Kelley, Barnes et al. 2013), which provides optimized com-\n\n---\n\nbundle optimization. In IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition (CVPR).\nZhang, H., Dauphin, Y . N., and Ma, T. (2019). Fixup initialization: Residual learning without nor-\nmalization. In International Conference on Learning Representations (ICLR).\nZhang, H., Cisse, M., Dauphin, Y . N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations (ICLR).\n\n---\n\nare all examples ofdiscriminative systems that never form an explicitgenerative model of the\nquantities they are trying to estimate (Bishop 2006, Section 1.5; Murphy 2012, Section 8.6).\nIn addition to the potential beneﬁts of generative models discussed in these two textbooks,\nGoodfellow (2016) and Kingma and Welling (2019) list some additional ones, such as the\nability to visualize our assumptions about our unknowns, training with missing or incom-\npletely labeled data, and the ability to generate multiple, alternative, results.\nIn computer graphics, which is sometimes called image synthesis (as opposed to the im-\nage understanding or image analysis we do in computer vision), the ability to easily gener-\nate realistic random images and models has long been an essential tool. Examples of such\nalgorithms include texture synthesis and style transfer, which we study in more detail in Sec-\ntion 10.5, as well as fractal terrain (Fournier, Fussel, and Carpenter 1982) and tree generation\n(Prusinkiewicz and Lindenmayer 1996). Examples of deep neural networks being used to\ngenerate such novel images, often under user control, are shown in Figures 5.60 and 10.58.\nRelated techniques are also used in the nascent ﬁeld of neural rendering, which we discuss\nin Section 14.6.\nHow can we unlock the demonstrated power of deep neural networks to capture seman-\ntics in order to visualize sample images and generate new ones? One approach could be to\n\n---\n\n12.5 Global optimization 775\nimage. Figure 12.13 schematically shows how DP works, while Figure 12.5f shows a real\nDSI slice over which the DP is applied.\nTo implement dynamic programming for a scanline y, each entry (state) in a 2D cost\nmatrix D(m,n) is computed by combining its DSI matching cost value with one of its prede-\ncessor cost values while also including a ﬁxed penalty for occluded pixels. The aggregation\nrules corresponding to Figure 12.13b are given by Kolmogorov, Criminisi et al. (2006), who\nalso use a two-state foreground–background model for bi-layer segmentation.\nProblems with dynamic programming stereo include the selection of the right cost for oc-\ncluded pixels and the difﬁculty of enforcing inter-scanline consistency, although several meth-\nods propose ways of addressing the latter (Ohta and Kanade 1985; Belhumeur 1996; Cox,\nHingorani et al. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999; Kolmogorov,\nCriminisi et al. 2006). Another problem is that the dynamic programming approach requires\nenforcing the monotonicity or ordering constraint (Yuille and Poggio 1984). This constraint\nrequires that the relative ordering of pixels on a scanline remain the same between the two\nviews, which may not be the case in scenes containing narrow foreground objects.\nAn alternative to traditional dynamic programming, introduced by Scharstein and Szeliski\n(2002), is to neglect the vertical smoothness constraints in (12.9) and simply optimize inde-",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar que es el background replacement y cuál técnica se usa para esta tarea?",
                "answer": " Background replacement, también conocido como video matting, es la técnica utilizada para separar objetos en un vídeo de su fondo. La técnica descrita por Chuang, Agarwala et al. (2002) consiste en primero eliminar los objetos foreground usando una malla de basura conservadora y luego alinear y compositar las placas de fondo resultantes para obtener una estimación de fondo de alta calidad. Además, se describe cómo se pueden interpolar mapas de trímeros dibujados en pocas keyframes a los marcos intermedios usando flujo óptico bidireccional.\n\nOtras aproximaciones a la técnica de video matting, como el rotoscoping que involucra dibujar curvas o trazos en las keyframes de un vídeo (Agarwala, Hertzmann et al. 2004; Wang, Bhat et al. 2005), son discutidas en el artículo de revisión sobre video matting por Wang y Cohen (2009).",
                "context": "background may get revealed in preceding or subsequent frames.\nChuang, Agarwala et al. (2002) describe a nice approach to this video matting problem,\nwhere foreground objects are ﬁrst removed using a conservative garbage matte and the re-\nsulting background plates are aligned and composited to yield a high-quality background\nestimate. They also describe how trimaps drawn at sparse keyframes can be interpolated to\nin-between frames using bi-direction optical ﬂow. Alternative approaches to video matting,\nsuch as rotoscoping, which involves drawing curves or strokes in video sequence keyframes\n(Agarwala, Hertzmann et al. 2004; Wang, Bhat et al. 2005), are discussed in the matting\nsurvey paper by Wang and Cohen (2009). There is also a newer dataset of carefully matted\nstop-motion animation videos created by Erofeev, Gitman et al. (2015).29\nSince the original development of video matting techniques, improved algorithms have\nbeen developed for both interactive and fully automated video object segmentation, as dis-\n29https://videomatting.com\n\n---\n\n3.8 Exercises 181\nEx 3.4: Blue screen matting. Set up a blue or green background, e.g., by buying a large\npiece of colored posterboard. Take a picture of the empty background, and then of the back-\nground with a new object in front of it. Pull the matte using the difference between each\ncolored pixel and its assumed corresponding background pixel, using one of the techniques\ndescribed in Section 3.1.3 or by Smith and Blinn (1996).\nEx 3.5: Difference keying. Implement a difference keying algorithm (see Section 3.1.3)\n(Toyama, Krumm et al. 1999), consisting of the following steps:\n1. Compute the mean and variance (or median and robust variance) at each pixel in an\n“empty” video sequence.\n2. For each new frame, classify each pixel as foreground or background (set the back-\nground pixels to RGBA=0).\n3. (Optional) Compute the alpha channel and composite over a new background.\n4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\ncomponents (Section 3.3.3), compute their centroids, and track them from frame to\nframe. Use this to build a “people counter”.\nEx 3.6: Photo effects. Write a variety of photo enhancement or effects ﬁlters: contrast,\nsolarization (quantization), etc. Which ones are useful (perform sensible corrections) and\nwhich ones are more creative (create unusual images)?\nEx 3.7: Histogram equalization. Compute the gray level (luminance) histogram for an im-\n\n---\n\numetric 3D reconstruction techniques (Section 12.7.3) (Szeliski 1993; Seitz and Dyer 1997;\nSeitz, Curless et al. 2006). It can also be used with a panning camera where the background\nis composited from frames where the foreground has been removed using a garbage matte\n(Section 10.4.5) (Chuang, Agarwala et al. 2002). Another application is the detection of vi-\n25See the alpha matting evaluation website at http://alphamatting.com.\n\n---\n\nconstrained least squares formulation similar to Baker, Szeliski, and Anandan (1998), where\nthe over compositing operator is replaced with addition.\nFigures 9.16 and 9.17 show the results of applying these techniques to two different pic-\nture frames with reﬂections. Notice how, in the second sequence, the amount of reﬂected light\nis quite low compared to the transmitted light (the picture of the girl) and yet the algorithm is\nstill able to recover both layers.\n\n---\n\ndates back to early work in stereo matching (Marr and Poggio 1976), while the concept of a\ndisparity space image (volume) is generally associated with Yang, Yuille, and Lu (1993) and\nIntille and Bobick (1994).\n12.1.2 Plane sweep\nAn alternative to pre-rectifying the images before matching is to sweep a set of planes through\nthe scene and to measure the photoconsistency of different images as they are re-projected\nonto these planes (Figure 12.6). This process is commonly known as the plane sweep algo-\nrithm (Collins 1996; Szeliski and Golland 1999; Saito and Kanade 1999).\n3The term disparity was ﬁrst introduced in the human vision literature to describe the difference in location of\ncorresponding features seen by the left and right eyes (Marr 1982). Horizontal disparity is the most commonly\n\n---\n\nThe algorithm therefore prefers removing regions that are near the edge of the image, which\nreduces the likelihood that partially visible objects will appear in the ﬁnal composite. (It is\nalso possible to infer which object in a region of difference is the foreground object by the\n“edginess” (pixel differences) across the ROD boundary, which should be higher when an\nobject is present (Herley 2005).) Once the desired excess regions of difference have been\nremoved, the ﬁnal composite can be created by feathering (Figure 8.14f).\nA different approach to pixel selection and seam placement is described by Agarwala,\nDontcheva et al. (2004). Their system computes the label assignment that optimizes the sum\nof two objective functions. The ﬁrst is a per-pixel image objective that determines which\npixels are likely to produce good composites,\nED =\n∑\nx\nD(x,l(x)), (8.72)\nwhere D(x,l) is the data penalty associated with choosing imagelat pixel x. In their system,\nusers can select which pixels to use by “painting” over an image with the desired object or",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Que es la reconstrucción de superficies?",
                "answer": " La reconstrucción de superficies se refiere a la técnica utilizada para crear modelos tridimensionales de una superficie basándose en datos obtenidos de imágenes o medidas. En el contexto proporcionado, se menciona que la reconstrucción de superficies es un tema importante en la imagen-based rendering y se discute su uso para la reconstrucción de escenas tridimensionales (Zhu y Kanade, 2008). También se menciona el sistema Urbanscan de Pollefeys, Nist'er et al. (2008) que reconstruye modelos texturizados en 3D de calles de la ciudad a partir de videos obtenidos con un vehículo equipado con GPS.",
                "context": "call “Priority-BP”. Wei, Lefebvre et al. (2009) present a comprehensive survey of work in\nexemplar-based texture synthesis through 2009.\n10.5.1 Application: Hole ﬁlling and inpainting\nFilling holes left behind when objects or defects are excised from photographs, which is\nknown as inpainting, is one of the most common applications of texture synthesis. Such\n\n---\n\nexamines the topic of appearance modeling, i.e., techniques for estimating the texture maps,\nalbedos, or even sometimes completebi-directional reﬂectance distribution functions(BRDFs)\nthat describe the appearance of 3D surfaces.\nIn Chapter 14, we discuss the large number of image-based rendering techniques that\nhave been developed in the last three decades, including simpler techniques such as view in-\nterpolation (Section 14.1), layered depth images (Section 14.2), and sprites and layers (Sec-\ntion 14.2.1), as well as the more general framework of light ﬁelds and Lumigraphs (Sec-\ntion 14.3) and higher-order ﬁelds such as environment mattes (Section 14.4). Applications of\nthese techniques include navigating 3D collections of photographs using photo tourism.\nNext, we discuss video-based rendering, which is the temporal extension of image-based\nrendering. The topics we cover include video-based animation (Section 14.5.1), periodic\nvideo turned into video textures (Section 14.5.2), and 3D video constructed from multiple\nvideo streams (Section 14.5.4). Applications of these techniques include animating still im-\nages and creating home tours based on 360° video. We ﬁnish the chapter with an overview of\nthe new emerging ﬁeld of neural rendering.\nTo support the book’s use as a textbook, the appendices and associated website contain\nmore detailed mathematical topics and additional material. Appendix A covers linear algebra\n\n---\n\nrectangle shapes and nesting structures (between rectangles and vanishing points) to infer\nthe most likely assignment of line segments to rectangles (Figure 7.51c). The idea of using\nregular, repetitive structures as part of the modeling process is now being called holistic 3D\nreconstruction (Zhou, Furukawa, and Ma 2019; Zhou, Furukawaet al. 2020; Pintore, Mura et\nal. 2020) and will be discussed in more detail in Section 13.6.1 on modeling 3D architecture.\n7.5 Segmentation\nImage segmentation is the task of ﬁnding groups of pixels that “go together”. In statistics and\nmachine learning, this problem is known as cluster analysis or more simply clustering and is\na widely studied area with hundreds of different algorithms (Jain and Dubes 1988; Kaufman\nand Rousseeuw 1990; Jain, Duin, and Mao 2000; Jain, Topchy et al. 2004; Xu and Wunsch\n2005). We’ve already discussed general vector-space clustering algorithms in Section 5.2.1.\nThe main difference between clustering and segmentation is that the former usually ignores\npixel layout and neighborhoods, while the latter relies heavily on spatial cues and constraints.\nIn computer vision, image segmentation is one of the oldest and most widely studied prob-\nlems (Brice and Fennema 1970; Pavlidis 1977; Riseman and Arbib 1977; Ohlander, Price,\nand Reddy 1978; Rosenfeld and Davis 1979; Haralick and Shapiro 1985). Early techniques\noften used region splitting or merging (Brice and Fennema 1970; Horowitz and Pavlidis 1976;\n\n---\n\numetric 3D reconstruction techniques (Section 12.7.3) (Szeliski 1993; Seitz and Dyer 1997;\nSeitz, Curless et al. 2006). It can also be used with a panning camera where the background\nis composited from frames where the foreground has been removed using a garbage matte\n(Section 10.4.5) (Chuang, Agarwala et al. 2002). Another application is the detection of vi-\n25See the alpha matting evaluation website at http://alphamatting.com.\n\n---\n\nexample of a two-layer decomposition reconstructed from a short video clip, which can be\nre-rendered from novel views by adding warped versions of the two layers (each of which\nhas its own depth map). When the reﬂective surface is curved, a quasi-stable virtual image\nmay still be available, although this depends on the local variations in principal curvatures\n(Swaminathan, Kang et al. 2002; Criminisi, Kang et al. 2005). The modeling of reﬂections\nis one of the advantages attributed to layered representations such as MPIs (Zhou, Tucker et\nal. 2018; Broxton, Flynn et al. 2020), although in these papers over compositing is still used,\nwhich results in plausible but not physically correct renderings.\n14.2.2 Application: 3D photography\nThe desire to capture and view photographs of the world in 3D prompted the development\nof stereo cameras and viewers in the mid-1800s (Luo, Kong et al. 2020) and more recently\n4Exploring the interactive 3D videos on the authors’ websites, e.g., https://augmentedperception.github.io/\ndeepviewvideo, is a good way to get a sense of this new medium.\n\n---\n\nPauly, Mitra et al. 2008; Sinha, Steedly et al. 2008). The combination of structured elements\nsuch as parallel lines, junctions, and rectangles with full axis-aligned 3D models for the\nmodeling of architectural environments has recently been called holistic 3D reconstruction.\nMore details can be found in the recent tutorial by Zhou, Furukawa, and Ma (2019), workshop\n(Zhou, Furukawa et al. 2020), and state-of-the-art report by Pintore, Mura et al. (2020).\nThe combination of all these techniques now makes it possible to reconstruct the struc-\nture of large 3D scenes (Zhu and Kanade 2008). For example, the Urbanscan system of\nPollefeys, Nist ´er et al. (2008) reconstructs texture-mapped 3D models of city streets from\nvideos acquired with a GPS-equipped vehicle. To obtain real-time performance, they use\nboth optimized online structure-from-motion algorithms, as well as GPU implementations of\nplane-sweep stereo aligned to dominant planes and depth map fusion. Cornelis, Leibe et al.\n(2008) present a related system that also uses plane-sweep stereo (aligned to vertical build-\ning fac ¸ades) combined with object recognition and segmentation for vehicles. Mi ˇcuˇs´ık and\nKoˇseck´a (2009) build on these results using omni-directional images and superpixel-based\nstereo matching along dominant plane orientations. Reconstruction directly from active range",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar el concepto de SLAM?",
                "answer": " El concepto de Simultaneous Localization and Mapping (SLAM) se refiere a una técnica utilizada en robótica y computación visual para que un agente autónomo, como un robot o un vehículo, pueda construir un mapa del entorno mientras se mueve por él. Al mismo tiempo, el agente localiza su propia posición dentro de ese mapa.\n\nEn SLAM, el agente utiliza sensores para recopilar información sobre su entorno y procesa esa información para crear un modelo tridimensional del mundo alrededor de él. Este proceso se conoce como \"mapping\". Al mismo tiempo, el agente utiliza la información del mapa para determinar su propia posición en el espacio. Este proceso se conoce como \"localization\".\n\nSLAM es importante porque permite a los agentes autónomos navegar y operar en entornos desconocidos sin necesidad de una guía humana o un mapa previamente creado. Algunas aplicaciones de SLAM incluyen la navegación autónoma, la exploración de entornos peligrosos, el reconocimiento de imágenes en tiempo real y la creación de mapas 3D para uso en videojuegos o simulaciones.\n\nAlgunos de los hitos importantes en SLAM incluyen la aplicación de SLAM a cámaras monoculares (MonoSLAM) (Davison, Reid et al., 2007), y el tracking y mapping paralelo (PTAM) (Klein y Murray, 2007).",
                "context": "applications such as autonomous navigation (Section 11.5.1) and real-time augmented reality\n(Section 11.5.2).\nSome of the important milestones in SLAM include:\n• the application of SLAM to monocular cameras (MonoSLAM) (Davison, Reid et al.\n2007);\n• parallel tracking and mapping (PTAM) (Klein and Murray 2007), which split the front\nend (tracking) and back end (mapping) processes (Figure 11.24) onto two separate\nthreads running at different rates (Figure 11.27) and then implemented the whole pro-\ncess on a camera phone (Klein and Murray 2009);\n\n---\n\n11 Structure from motion and SLAM 683\n11.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 740\n11.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743\n\n---\n\n11.5 Simultaneous localization and mapping (SLAM) 735\nFigure 11.24 The architecture of the LSD-SLAM system (Engel, Sch ¨ops, and Cremers\n2014) © 2014 Springer, showing the front end, which does the tracking, data association,\nand local 3D pose and structure (depth map) updating, and the back end, which does global\nmap optimization.\nrange ﬁnders, or stereo matching, to estimate local 3D geometry, which could then be fused\ninto a 3D model. Newer techniques can perform the same task based purely on visual feature\ntracking from a monocular camera (Davison, Reid et al. 2007). Good introductory tutorials\ncan be found in Durrant-Whyte and Bailey (2006) and Bailey and Durrant-Whyte (2006),\nwhile more comprehensive surveys of more recent techniques are presented in (Fuentes-\nPacheco, Ruiz-Ascencio, and Rend´on-Mancha 2015) and Cadena, Carlone et al. (2016).\nSLAM differs from bundle adjustment in two fundamental aspects. First, it allows for a\nvariety of sensing devices, instead of just being restricted to tracked or matched feature points.\nSecond, it solves the localization problem online, i.e., with no or very little lag in providing\nthe current sensor pose. This makes it the method of choice for both time-critical robotics\napplications such as autonomous navigation (Section 11.5.1) and real-time augmented reality\n(Section 11.5.2).\nSome of the important milestones in SLAM include:\n• the application of SLAM to monocular cameras (MonoSLAM) (Davison, Reid et al.\n2007);\n\n---\n\nChapter 11\nStructure from motion and SLAM\n11.1 Geometric intrinsic calibration . . . . . . . . . . . . . . . . . . . . . . . . . 685\n11.1.1 Vanishing points . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687\n11.1.2 Application: Single view metrology . . . . . . . . . . . . . . . . . . 688\n11.1.3 Rotational motion . . . . . . . . . . . . . . . . . . . . . . . . . . . 689\n11.1.4 Radial distortion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691\n11.2 Pose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693\n11.2.1 Linear algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693\n11.2.2 Iterative non-linear algorithms . . . . . . . . . . . . . . . . . . . . . 695\n11.2.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 698\n11.2.4 Triangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701\n11.3 Two-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 703\n11.3.1 Eight, seven, and ﬁve-point algorithms . . . . . . . . . . . . . . . . . 703\n11.3.2 Special motions and structures . . . . . . . . . . . . . . . . . . . . . 708\n11.3.3 Projective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 710\n11.3.4 Self-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712\n11.3.5 Application: View morphing . . . . . . . . . . . . . . . . . . . . . . 714\n11.4 Multi-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . 715\n\n---\n\nof structure from motion to large image sets. Additional techniques for efﬁciently solving\nlarge structure from motion and SLAM systems can be found in the survey by Dellaert and\nKaess (2017); Dellaert (2021).\nWhile bundle adjustment and other robust non-linear least squares techniques are the\nmethods of choice for most structure-from-motion problems, they suffer from initialization\nproblems, i.e., they can get stuck in local energy minima if not started sufﬁciently close\nto the global optimum. Many systems try to mitigate this by being conservative in what\nreconstruction they perform early on and which cameras and points they add to the solution\n(Section 11.4.6). An alternative, however, is to re-formulate the problem using a norm that\nsupports the computation of global optima.\nKahl and Hartley (2008) describe techniques for using L∞ norms in geometric recon-\nstruction problems. The advantage of such norms is that globally optimal solutions can be\nefﬁciently computed using second-order cone programming (SOCP). The disadvantage is that\nL∞norms are particularly sensitive to outliers and so must be combined with good outlier\nrejection techniques before they can be used.\nA large number of high-quality open source bundle adjustment packages have been de-\nveloped, including the Ceres Solver, 20 Multicore Bundle Adjustment (Wu, Agarwal et al.\n2011),21 the Sparse Levenberg-Marquardt based non-linear least squares optimizer and bun-\n\n---\n\nDontcheva et al. (2004) can be written as\nS(x,y,lx,ly) = ∥˜Ilx(x) −˜Ily(x)∥+ ∥˜Ilx(y) −˜Ily(y)∥. (8.74)\nMore sophisticated seam penalties can also look at image gradients or the presence of image\nedges (Agarwala, Dontcheva et al. 2004). Seam penalties are widely used in other computer\nvision applications such as stereo matching (Boykov, Veksler, and Zabih 2001) to give the\nlabeling function its coherence or smoothness. An alternative approach, which places seams\nalong strong consistent edges in overlapping images using a watershed computation is de-\nscribed by Soille (2006).",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Sirve la contabilidad general en el ámbito de Computer Vision?",
                "answer": " No, la contabilidad general no se utiliza directamente en el ámbito de Computer Vision. La contabilidad general es una disciplina que se encarga de registrar, clasificar y procesar los datos financieros de una empresa o organización. En contraste, el campo de Computer Vision se centra en la capacidad de computadoras para procesar y analizar imágenes y videos, con aplicaciones que van desde la reconocimiento facial hasta la detección de objetos y la interpretación de imágenes. Sin embargo, la contabilidad general puede ser relevante en el ámbito de Computer Vision en cuanto a la gestión financiera de proyectos relacionados con esta área, como el desarrollo de software o la investigación científica.",
                "context": "826 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nfaces (Stollnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren and\nWeimer 2001; Peters and Reif 2008), enable not only the creation of highly detailed models\nbut also processing operations, such as interpolation (Section 13.3.1), fairing or smoothing,\nand decimation and simpliﬁcation (Section 13.3.2). We also examine discrete point-based\nrepresentations (Section 13.4) and volumetric representations (Section 13.5).\n13.3.1 Surface interpolation\nOne of the most common operations on surfaces is their reconstruction from a set of sparse\ndata constraints, i.e., scattered data interpolation, which we covered in Section 4.1. When\nformulating such problems, surfaces may be parameterized as height ﬁelds f(x), as 3D para-\nmetric surfaces f(x), or as non-parametric models such as collections of triangles.\nIn Section 4.2, we saw how two-dimensional function interpolation and approximation\nproblems {di}→ f(x) could be cast as energy minimization problems using regularization\n(4.18–4.23). Such problems can also specify the locations of discontinuities in the surface as\nwell as local orientation constraints (Terzopoulos 1986b; Zhang, Dugas-Phocionet al. 2002).\nOne approach to solving such problems is to discretize both the surface and the energy\non a discrete grid or mesh using ﬁnite element analysis (4.24–4.27) (Terzopoulos 1986b).\n\n---\n\n408 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nthe image, such as the ones shown in Figure 6.48c. A lot of this work started with the cre-\nation of the Visual Question Answering (VQA) dataset (Antol, Agrawal et al. 2015), which\nspurred a large amount of subsequent research. The following year, VQA v2.0 improved\nthis dataset by creating a balanced set of image pairs, where each question had different an-\nswers in the two images (Goyal, Khot et al. 2017).29 This dataset was further extended to\nreduce the inﬂuence of prior assumptions and data distributions and to encourage answers to\nbe grounded in the images (Agrawal, Batra et al. 2018).\nSince then, many additional VQA datasets have been created. These include the VCR\ndataset for visual commonsense reasoning (Zellers, Bisket al. 2019) and the GQA dataset and\nmetrics for evaluating visual reasoning and compositional question answering (Hudson and\nManning 2019), which is built on top of the information about objects, attributes, and relations\nprovided through the Visual Genome scene graphs (Krishna, Zhu et al. 2017). A discussion\nof these and other datasets for VQA can be found in the CVPR 2020 tutorial by Gan (2020),\nincluding datasets that test visual grounding and referring expression comprehension, visual\nentailment, using external knowledge, reading text, answering sub-questions, and using logic.\nSome of these datasets are summarized in Table 6.4.\n\n---\n\n548 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nin more detail in Appendix B.3 and in monographs and review articles on this topic (Huber\n1981; Hampel, Ronchetti et al. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan\n1996; Stewart 1999). The most commonly used robust initialization technique in computer\nvision is RANdom SAmple Consensus (RANSAC) (Fischler and Bolles 1981), which has\nspawned a series of more efﬁcient variants (Torr and Zisserman 2000; Nist´er 2003; Chum and\nMatas 2005; Raguram, Chum et al. 2012; Brachmann, Krull et al. 2017; Barath and Matas\n2018; Barath, Matas, and Noskova 2019; Brachmann and Rother 2019). The MAGSAC++\npaper by Barath, Noskova et al. (2020) compares many of these variants.\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\n\n---\n\n860 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n1178 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n416 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            }
        ]
    },
    "Fixed": {
        "avg_faithfulness": 0.3333333333333333,
        "avg_context_relevance": 0.3333333333333333,
        "avg_answer_relevance": 1.0,
        "details": [
            {
                "question": "¿Cuál es el uso más común que se le da a la técnica Pose Estimation?",
                "answer": " The most common use of the Pose Estimation technique is in human pose estimation, which is used in various applications such as animation, sports analysis, and surveillance systems. This technique helps to determine the position and orientation of different body parts of a person in an image or video sequence. It's particularly useful for creating realistic animations in movies, games, and virtual reality, as well as for analyzing human movements in sports, dance, or physical therapy. In surveillance systems, it can be used for identifying individuals, tracking their behavior, and detecting abnormal activities.",
                "context": "11.2 Pose estimation 693\n11.2 Pose estimation\nA particular instance of feature-based alignment, which occurs very often, is estimating an\nobject’s 3D pose from a set of 2D point projections. This pose estimation problem is also\nknown as extrinsic calibration, as opposed to the intrinsic calibration of internal camera pa-\nrameters such as focal length, which we discuss in Section 11.1. The problem of recovering\npose from three correspondences, which is the minimal amount of information necessary,\nis known as the perspective-3-point-problem (P3P),2 with extensions to larger numbers of\npoints collectively known as PnP (Haralick, Lee et al. 1994; Quan and Lan 1999; Gao, Hou\net al. 2003; Moreno-Noguer, Lepetit, and Fua 2007; Persson and Nordberg 2018).\nIn this section, we look at some of the techniques that have been developed to solve such\nproblems, starting with thedirect linear transform(DLT), which recovers a 3×4 camera ma-\ntrix, followed by other “linear” algorithms, and then looking at statistically optimal iterative\nalgorithms.\n11.2.1 Linear algorithms\nThe simplest way to recover the pose of the camera is to form a set of rational linear equations\nanalogous to those used for 2D motion estimation (8.19) from the camera matrix form of\nperspective projection (2.55–2.56),\nxi = p00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(11.11)\nyi = p10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n, (11.12)\nwhere (xi,yi) are the measured 2D feature locations and (Xi,Yi,Zi) are the known 3D\nfeature locations (Figure 11.6). As with (8.21), this system of equations can be solved in a\nlinear fashion for the unknowns in the camera matrix P by multiplying the denominator on\nboth sides of the equation.Because P is unknown up to a scale, we can either ﬁx one of the\nentries, e.g., p23 = 1, or ﬁnd the smallest singular vector of the set of linear equations. The\nresulting algorithm is called the direct linear transform (DLT) and is commonly attributed\nto Sutherland (1974). (For a more in-depth discussion, see Hartley and Zisserman (2004).)\nTo compute the 12 (or 11) unknowns in P, at least six correspondences between 3D and 2D\nlocations must be known.\nAs with the case of estimating homographies (8.21–8.23), more accurate results for the\nentries in P can be obtained by directly minimizing the set of Equations (11.11–11.12) using\n2The “3-point” algorithms actually require a 4th point to resolve a 4-way ambiguity.\n\n---\n\n11.2 Pose estimation 695\npj (Figure 11.6).\nA full derivation of this approach can be found in the ﬁrst edition of this book (Szeliski\n2010, Section 6.2.1) and also in (Quan and Lan 1999), where the authors provide accuracy\nresults for this and other techniques, which use fewer points but require more complicated\nalgebraic manipulations. The paper by Moreno-Noguer, Lepetit, and Fua (2007) reviews\nother alternatives and also gives a lower complexity algorithm that typically produces more\naccurate results. An even more recent paper by Terzakis and Lourakis (2020) reviews papers\npublished in the last decade.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 11.4.5), it is prudent to optimize\nthe initial estimates from PnP using the iterative technique described in Section 11.2.2. An\nalternative pose estimation algorithm involves starting with a scaled orthographic projection\nmodel and then iteratively reﬁning this initial estimate using a more accurate perspective\nprojection model (DeMenthon and Davis 1995). The attraction of this model, as stated in the\npaper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\nCNN-based pose estimation\nAs with other areas on computer vision, deep neural networks have also been applied to pose\nestimation. Some representative papers include Xiang, Schmidt et al. (2018), Oberweger,\nRad, and Lepetit (2018), Hu, Hugonot et al. (2019), Peng, Liu et al. (2019), and (Hu, Fua\net al. 2020) for object pose estimation, and papers such as Kendall and Cipolla (2017) and\nKim, Dunn, and Frahm (2017) discussed in Section 11.2.3 on location recognition. There\nis also a very active community around estimating pose from RGB-D images, with the most\nrecent papers (Hagelskjær and Buch 2020; Labb ´e, Carpentier et al. 2020) evaluated on the\nBOP (Benchmark for 6DOF Object Pose) (Hodaˇn, Michel et al. 2018).4\n11.2.2 Iterative non-linear algorithms\nThe most accurate and ﬂexible way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R,t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher and\nWitkin 1992). We can write the projection equations as\nxi = f(pi; R,t,K) (11.14)\n4https://bop.felk.cvut.cz/challenges/bop-challenge-2020, https://cmp.felk.cvut.cz/sixd/workshop 2020\n\n---\n\n3.1 Point operators 113\n(a) (b) (c) (d)\nFigure 3.4 Image matting and compositing (Chuang, Curless et al. 2001) © 2001 IEEE:\n(a) source image; (b) extracted foreground object F; (c) alpha matte αshown in grayscale;\n(d) new composite C.\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’shue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\nAs discussed in Section 2.3.2, chromaticity coordinates (2.105) or even simpler color ra-\ntios (2.117) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figures\n2.33f–h show some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 ×3 color twist transform matrix.\nExercises 2.8 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n\n---\n\n13.9 Exercises 857\nmatic Face and Gesture Recognition (FG) and IEEE Workshop on Analysis and Modeling of\nFaces and Gestures (AMFG). Two recent survey papers on 3D face modeling and tracking\nare Zollh¨ofer, Thies et al. (2018) and Egger, Smithet al. (2020), while surveys on the topic of\nwhole body modeling and tracking include Forsyth, Arikan et al. (2006), Moeslund, Hilton,\nand Kr¨uger (2006), and Sigal, Balan, and Black (2010).\nSome representative papers on recovering texture maps from multiple color and RGB-D\nimages include Gal, Wexler et al. (2010), Waechter, Moehrle, and Goesele (2014), Zhou and\nKoltun (2014), and Lee, Haet al. (2020) as well as Zollh¨ofer, Stotko et al. (2018, Section 4.1).\nThe more complex process of recovering spatially varying BRDFs is covered in surveys by\nDorsey, Rushmeier, and Sillion (2007) and Weyrich, Lawrence et al. (2009). More recent\ntechniques that can do this using fewer images and RGB-D images include Aittala, Weyrich,\nand Lehtinen (2015), Li, Sunkavalli, and Chandraker (2018), Schmitt, Donne et al. (2020),\nand Boss, Jampani et al. (2020) and the survey by Zollh¨ofer, Stotko et al. (2018).\n13.9 Exercises\nEx 13.1: Shape from focus. Grab a series of focused images with a digital SLR set to man-\nual focus (or get one that allows for programmatic focus control) and recover the depth of an\nobject.\n1. Take some calibration images, e.g., of a checkerboard, so that you can compute a map-\nping between the amount of defocus and the focus setting.\n2. Try both a fronto-parallel planar target and one which is slanted so that it covers the\nworking range of the sensor. Which one works better?\n3. Now put a real object in the scene and perform a similar focus sweep.\n4. For each pixel, compute the local sharpness and ﬁt a parabolic curve over focus settings\nto ﬁnd the most in-focus setting.\n5. Map these focus settings to depth and compare your result to ground truth. If you are\nusing a known simple object, such as a sphere or cylinder (a ball or a soda can), it’s\neasy to measure its true shape.\n6. (Optional) See if you can recover the depth map from just two or three focus settings.\n7. (Optional) Use an LCD projector to project artiﬁcial texture onto the scene. Use a pair\nof cameras to compare the accuracy of your shape from focus and shape from stereo\ntechniques.\n\n---\n\n848 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nFigure 13.29 Estimating human shape and pose from a single image using a parametric\n3D model (Guan, Weiss et al. 2009) © 2009 IEEE.\nscans using semi-automated marker placement. The registered datasets are used to model the\nvariation in shape as a function of personal characteristics and skeletal pose, e.g., the bulging\nof muscles as certain joints are ﬂexed (Figure 13.29, top row). The resulting system can then\nbe used for shape completion, i.e., the recovery of a full 3D mesh model from a small number\nof captured markers, by ﬁnding the best model parameters in both shape and pose space that\nﬁt the measured data.\nBecause it is constructed completely from scans of people in close-ﬁtting clothing and\nuses a parametric shape model, the SCAPE system cannot cope with people wearing loose-\nﬁtting clothing. B ˘alan and Black (2008) overcome this limitation by estimating the body\nshape that ﬁts within the visual hull of the same person observed in multiple poses, while\nVlasic, Baran et al. (2008) adapt an initial surface mesh ﬁtted with a parametric shape model\nto better match the visual hull.\nWhile the preceding body ﬁtting and pose estimation systems use multiple views to es-\ntimate body shape, Guan, Weiss et al. (2009) ﬁt a human shape and pose model to a single\nimage of a person on a natural background. Manual initialization is used to estimate a rough\npose (skeleton) and height model, and this is then used to segment the person’s outline using\nthe Grab Cut segmentation algorithm (Section 4.3.2). The shape and pose estimate are then\nreﬁned using a combination of silhouette edge cues and shading information (Figure 13.29).\nThe resulting 3D model can be used to create novel animations.\n\n---\n\n6.6 Vision and language 407\ntext-image pairs on the internet) and compute to achieve astonishing qualitative results (Fig-\nure 6.49).28 Their approach produces promising results for generalizing beyond training data,\neven compositionally piecing together objects that are not often related (e.g., an armchair and\nan avocado), producing many styles (e.g., painting, cartoon, charcoal drawings), and working\nreasonably well with difﬁcult objects (e.g., mirrors or text).\nThe model for DALL·E consists of two components: a VQ-V AE-2 (Section 5.5.4) and a\ndecoder transformer (Section 5.5.3). The text is tokenized into 256 tokens, each of which is\none of 16,384 possible vectors using a BPE-encoding (Sennrich, Haddow, and Birch 2015).\nThe VQ-V AE-2 uses a codebook of size 8,192 (signiﬁcantly larger than the codebook of size\n512 used in the original VQ-V AE-2 paper) to compress images as a 32 ×32 grid of vector\ntokens. At inference time, DALL·E uses a transformer decoder, which starts with the 256\ntext tokens to autoregressively predict the 32 ×32 grid of image tokens. Given such a grid,\nthe VQ-V AE-2 is able to use its decoder to generate the ﬁnal RGB image of size 256×256.\nTo achieve better empirical results, DALL·E generates 512 image candidates and reranks\nthem using CLIP (Radford, Kim et al. 2021), which determines how likely a given caption is\nassociated with a given image.\nAn intriguing extension of DALL·E is to use the VQ-V AE-2 encoder to predict a subset\nof the compressed image tokens. For instance, suppose we are given a text input and an\nimage. The text input can be tokenized into its 256 tokens, and one can obtain the 32 ×\n32 image tokens using the VQ-V AE-2 encoder. If we then discard the bottom half of the\nimage tokens, the transformer decoder can be used to autoregressively predict which tokens\nmight be there. These tokens, along with the non-discarded ones from the original image, can\nbe passed into the VQ-V AE-2 decoder to produce a completed image. Figure 6.49 (bottom\nright) shows how such a text and partial image prompt can be used for applications such as\nimage-to-image translation (Section 5.5.4).\nVisual Question Answering and Reasoning\nImage and video captioning are useful tasks that bring us closer to building artiﬁcially in-\ntelligent systems, as they demonstrate the ability to put together visual cues such as object\nidentities, attributes, and actions. However, it remains unclear if the system has understood\nthe scene at a deeper level and if it can reason about the constituent pieces and how they ﬁt\ntogether.\nTo address these concerns, researchers have been building visual question answering\n(VQA) systems, which require the vision algorithm to answer open-ended questions about\n28Play with the results at https://openai.com/blog/dall-e.",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿En que casos es útil la programación dinámica?",
                "answer": " La programación dinámica (PD) es útil en casos donde se necesita encontrar una solución óptima para un problema de optimización, especialmente cuando el número de opciones posibles es grande y la complejidad crece rápidamente con el tamaño del problema.\n\nEn general, los problemas que pueden ser resueltos mediante PD son aquellos que se pueden dividir en subproblemas más pequeños y donde cada subproblema puede ser resuelto de manera independiente. Además, la solución óptima para un problema debe poder construirse combinando las soluciones óptimas de sus subproblemas.\n\nAlgunos ejemplos de problemas que pueden ser resueltos mediante PD incluyen:\n\n* Problemas de programación lineal y no lineal\n* Problemas de planificación de recursos\n* Problemas de optimización de rutas\n* Problemas de juegos de decisión\n* Problemas de optimización de costes en gráficos\n\nPara más información sobre la programación dinámica, se puede consultar los siguientes artículos:\n\n* Cormen, T. H., Leiserson, C. E., Rivest, R. L., y Stein, C. (2009). Introducción a la algoritmia. McGraw-Hill Interamericana de España.\n* Aho, A. V., Hopcroft, J. E., y Ullman, J. D. (1974). Compiladores: Principios de diseño. Pearson Education.\n* Bellman, R. E. (1957). Dinámica matemática del control óptimo: Una visión general. Mathematics of Control, Signals and Systems, 1(1):7–35.",
                "context": "24 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nand still others as open-ended research problems that make for challenging ﬁnal projects.\nMotivated students who implement a reasonable subset of these exercises will, by the end of\nthe book, have a computer vision software library that can be used for a variety of interesting\ntasks and projects.\nIf the students or curriculum do not have a strong preference for programming languages,\nPython, with the NumPy scientiﬁc and array arithmetic library plus the OpenCV vision li-\nbrary, are a good environment to develop algorithms and learn about vision. Not only will the\nstudents learn how to program using array/tensor notation and linear/matrix algebra (which is\na good foundation for later use of PyTorch for deep learning), you can also prepare classroom\nassignments using Jupyter notebooks, giving you the option to combine descriptive tutorials,\nsample code, and code to be extended/modiﬁed in one convenient location.10\nAs this is a reference book, I try wherever possible to discuss which techniques and al-\ngorithms work well in practice, as well as provide up-to-date pointers to the latest research\nresults in the areas that I cover. The exercises can be used to build up your own personal\nlibrary of self-tested and validated vision algorithms, which is more worthwhile in the long\nterm (assuming you have the time) than simply pulling algorithms out of a library whose\nperformance you do not really understand.\nThe book begins in Chapter 2 with a review of the image formation processes that create\nthe images that we see and capture. Understanding this process is fundamental if you want\nto take a scientiﬁc (model-based) approach to computer vision. Students who are eager to\njust start implementing algorithms (or courses that have limited time) can skip ahead to the\nnext chapter and dip into this material later. In Chapter 2, we break down image formation\ninto three major components. Geometric image formation (Section 2.1) deals with points,\nlines, and planes, and how these are mapped onto images usingprojective geometry and other\nmodels (including radial lens distortion). Photometric image formation (Section 2.2) covers\nradiometry, which describes how light interacts with surfaces in the world, andoptics, which\nprojects light onto the sensor plane. Finally, Section 2.3 covers how sensors work, including\ntopics such as sampling and aliasing, color sensing, and in-camera compression.\nChapter 3 covers image processing, which is needed in almost all computer vision appli-\ncations. This includes topics such as linear and non-linear ﬁltering (Section 3.3), the Fourier\ntransform (Section 3.4), image pyramids and wavelets (Section 3.5), and geometric transfor-\nmations such as image warping (Section 3.6). Chapter 3 also presents applications such as\nseamless image blending and image morphing.\nChapter 4 begins with a new section on data ﬁtting and interpolation, which provides a\n10You may also be able to run your notebooks and train your models using the Google Colab service at https:\n//colab.research.google.com.\n\n---\n\n12.5 Global optimization 775\nimage. Figure 12.13 schematically shows how DP works, while Figure 12.5f shows a real\nDSI slice over which the DP is applied.\nTo implement dynamic programming for a scanline y, each entry (state) in a 2D cost\nmatrix D(m,n) is computed by combining its DSI matching cost value with one of its prede-\ncessor cost values while also including a ﬁxed penalty for occluded pixels. The aggregation\nrules corresponding to Figure 12.13b are given by Kolmogorov, Criminisi et al. (2006), who\nalso use a two-state foreground–background model for bi-layer segmentation.\nProblems with dynamic programming stereo include the selection of the right cost for oc-\ncluded pixels and the difﬁculty of enforcing inter-scanline consistency, although several meth-\nods propose ways of addressing the latter (Ohta and Kanade 1985; Belhumeur 1996; Cox,\nHingorani et al. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999; Kolmogorov,\nCriminisi et al. 2006). Another problem is that the dynamic programming approach requires\nenforcing the monotonicity or ordering constraint (Yuille and Poggio 1984). This constraint\nrequires that the relative ordering of pixels on a scanline remain the same between the two\nviews, which may not be the case in scenes containing narrow foreground objects.\nAn alternative to traditional dynamic programming, introduced by Scharstein and Szeliski\n(2002), is to neglect the vertical smoothness constraints in (12.9) and simply optimize inde-\npendent scanlines in the global energy function (12.7). The advantage of this scanline op-\ntimization algorithm is that it computes the same representation and minimizes a reduced\nversion of the same energy function as the full 2D energy function (12.7). Unfortunately, it\nstill suffers from the same streaking artifacts as dynamic programming. Dynamic program-\nming is also possible on tree structures, which can ameliorate the streaking (Veksler 2005).\nMuch higher quality results can be obtained by summing up the cumulative cost function\nfrom multiple directions, e.g, from the eight cardinal directions, N, E, W, S, NE, SE, SW,\nNW (Hirschm ¨uller 2008). The resulting semi-global matching (SGM) algorithm performs\nquite well and is extremely efﬁcient, enabling real-time low-power implementations (Gehrig,\nEberli, and Meyer 2009). Drory, Haubold et al. (2014) show that SGM is equivalent to early\nstopping for a particular variant of belief propagation. Semi-global matching has also been\nextended using learned components, e.g., SGM-Net (Seki and Pollefeys 2017), which uses a\nCNN to adjust transition costs, and SGM-Forest (Sch ¨onberger, Sinha, and Pollefeys 2018),\nwhich uses a random-forest classiﬁer to fuse disparity proposals from different directions.\n12.5.2 Segmentation-based techniques\nWhile most stereo matching algorithms perform their computations on a per-pixel basis, some\ntechniques ﬁrst segment the images into regions and then try to label each region with a\ndisparity.\n\n---\n\n326 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n• Residual connections (He, Zhang et al. 2016a) are employed after multi-headed atten-\ntion and after the ﬁnal MLP.\nDuring training, the biggest difference in the decoder is that some of the input vectors to\nself-attention may be masked out, which helps support parallel training in autoregressive\nprediction tasks. Further exposition of the details and implementation of the transformer\narchitecture is provided in Vaswani, Shazeer et al. (2017) and in the additional reading (Sec-\ntion 5.6).\nA key challenge of applying transformers to the image domain has to do with the size of\nimage input (Vaswani, Shazeer et al. 2017). Let N denote the length of the input, Ddenote\nthe number of dimensions for each input entry, andKdenote a convolution’s (on side) kernel\nsize.58 The number of ﬂoating point operations (FLOPs) required for self-attention is on\nthe order of O(N2D), whereas the FLOPs for a convolution operation is on the order of\nO(ND2K2). For instance, with an ImageNet image scaled to size 224 ×224 ×3, if each\npixel is treated independently, N = 224 ×224 = 50176 and D = 3. Here, a convolution is\nsigniﬁcantly more efﬁcient than self-attention. In contrast, applications like neural machine\ntranslation may only have N as the number of words in a sentence and Das the dimension\nfor each word embedding (Mikolov, Sutskeveret al. 2013), which makes self-attention much\nmore efﬁcient.\nThe Image Transformer (Parmar, Vaswani et al. 2018) was the ﬁrst attempt at applying\nthe full transformer model to the image domain, with many of the same authors that intro-\nduced the transformer. It used both an encoder and decoder to try and build an autoregressive\ngenerative model that predicts the next pixel, given a sequence of input pixels and all the\npreviously predicted pixels. (The earlier work on non-local networks by Wang, Girshick et\nal. (2018) also used ideas inspired by transformers, but with a simpler attention block and a\nfully two-dimensional setup.) Each vector input to the transformer corresponded to a single\npixel, which ultimately constrained them to generate small images (i.e., 32 ×32), since the\nquadratic cost of self-attention was too expensive otherwise.\nDosovitskiy, Beyer et al. (2021) had a breakthrough that allowed transformers to process\nmuch larger images. Figure 5.57 shows the diagram of the model, named the Vision Trans-\nformer (ViT). For the task of image recognition, instead of treating each pixel as a separate\ninput vector to the transformer, they divide an image (of size 224 ×224) into 196 distinct\n16 ×16 gridded image patches. Each patch is then ﬂattened, and passed through a shared\nembedding matrix, which is equivalent to a strided 16 ×16 convolution, and the results are\ncombined with a positional encoding vector and then passed to the transformer. Earlier work\n58In Section 5.4 on convolutional architectures, we useCto denote the number of channels instead ofDto denote\nthe embedding dimensions.\n\n---\n\nviii Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\ntechniques and the recent research literature. To this end, I have attempted wherever possible\nto at least cite the newest research in each sub-ﬁeld, even if the technical details are too\ncomplex to cover in the book itself.\nIn teaching our courses, we have found it useful for the students to attempt a number of\nsmall implementation projects, which often build on one another, in order to get them used to\nworking with real-world images and the challenges that these present. The students are then\nasked to choose an individual topic for each of their small-group, ﬁnal projects. (Sometimes\nthese projects even turn into conference papers!) The exercises at the end of each chapter\ncontain numerous suggestions for smaller mid-term projects, as well as more open-ended\nproblems whose solutions are still active research topics. Wherever possible, I encourage\nstudents to try their algorithms on their own personal photographs, since this better motivates\nthem, often leads to creative variants on the problems, and better acquaints them with the\nvariety and complexity of real-world imagery.\nIn formulating and solving computer vision problems, I have often found it useful to draw\ninspiration from four high-level approaches:\n• Scientiﬁc: build detailed models of the image formation process and develop mathe-\nmatical techniques to invert these in order to recover the quantities of interest (where\nnecessary, making simplifying assumptions to make the mathematics more tractable).\n• Statistical: use probabilistic models to quantify the prior likelihood of your unknowns\nand the noisy measurement processes that produce the input images, then infer the best\npossible estimates of your desired quantities and analyze their resulting uncertainties.\nThe inference algorithms used are often closely related to the optimization techniques\nused to invert the (scientiﬁc) image formation processes.\n• Engineering: develop techniques that are simple to describe and implement but that\nare also known to work well in practice. Test these techniques to understand their\nlimitation and failure modes, as well as their expected computational costs (run-time\nperformance).\n• Data-driven: collect a representative set of test data (ideally, with labels or ground-\ntruth answers) and use these data to either tune or learn your model parameters, or at\nleast to validate and quantify its performance.\nThese four approaches build on each other and are used throughout the book.\nMy personal research and development philosophy (and hence the exercises in the book)\nhave a strong emphasis on testing algorithms. It’s too easy in computer vision to develop an\n\n---\n\n11.4 Multi-frame structure from motion 723\nware24 as well as examples of their application25 on the web.\n11.4.4 Application: Match move\nOne of the neatest applications of structure from motion is to estimate the 3D motion of\na video or ﬁlm camera, along with the geometry of a 3D scene, in order to superimpose 3D\ngraphics or computer-generated images (CGI) on the scene. In the visual effects industry, this\nis known as the match move problem (Roble 1999), as the motion of the synthetic 3D camera\nused to render the graphics must be matched to that of the real-world camera. For very small\nmotions, or motions involving pure camera rotations, one or two tracked points can sufﬁce\nto compute the necessary visual motion. For planar surfaces moving in 3D, four points are\nneeded to compute the homography, which can then be used to insert planar overlays, e.g., to\nreplace the contents of advertising billboards during sporting events.\nThe general version of this problem requires the estimation of the full 3D camera pose\nalong with the focal length (zoom) of the lens and potentially its radial distortion parameters\n(Roble 1999). When the 3D structure of the scene is known ahead of time, pose estima-\ntion techniques such as view correlation (Bogart 1991) or through-the-lens camera control\n(Gleicher and Witkin 1992) can be used, as described in Section 11.4.4.\nFor more complex scenes, it is usually preferable to recover the 3D structure simultane-\nously with the camera motion using structure-from-motion techniques. The trick with using\nsuch techniques is that to prevent any visible jitter between the synthetic graphics and the\nactual scene, features must be tracked to very high accuracy and ample feature tracks must\nbe available in the vicinity of the insertion location. Some of today’s best known match\nmove software packages, such as the boujou package from 2d3, which won an Emmy award\nin 2002, originated in structure-from-motion research in the computer vision community\n(Fitzgibbon and Zisserman 1998).\n11.4.5 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997;\nWilson and Wehrwein 2020). An example of this is the classic bas-relief ambiguity, which\n24https://peterfalkingham.com/2020/07/10/free-and-commercial-photogrammetry-software-review-2020\n25https://beforesandafters.com/2020/07/06/tales-from-on-set-lidar-scanning-for-joker-and-john-wick-3, https://\nrd.nytimes.com/projects/reconstructing-journalistic-scenes-in-3d\n\n---\n\n1108 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nRadford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 . presented at\nICLR 2016.\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and Doll ´ar, P. (2020). Designing network\ndesign spaces. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nRagan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., and Amarasinghe, S. (2013). Halide: a\nlanguage and compiler for optimizing parallelism, locality, and recomputation in image processing\npipelines. In ACM SIGPLAN Notices, pp. 519–530.\nRaginsky, M. and Lazebnik, S. (2009). Locality-sensitive binary codes from shift-invariant kernels.\nIn Advances in Neural Information Processing Systems (NeurIPS).\nRaguram, R., Wu, C., Frahm, J.-M., and Lazebnik, S. (2011). Modeling and recognition of land-\nmark image collections using iconic scene graphs. International Journal of Computer Vision ,\n95(3):213–239.\nRaguram, R., Chum, O., Pollefeys, M., Matas, J., and Frahm, J.-M. (2012). USAC: a universal\nframework for random sample consensus. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35(8):2022–2038.\nRaman, S. and Chaudhuri, S. (2007). A matte-less, variational approach to automatic scene composit-\ning. In International Conference on Computer Vision (ICCV).\nRaman, S. and Chaudhuri, S. (2009). Bilateral ﬁlter based compositing for variable exposure photog-\nraphy. In Eurographics.\nRamanan, D. and Baker, S. (2009). Local distance functions: A taxonomy, new algorithms, and an\nevaluation. In International Conference on Computer Vision (ICCV).\nRamanan, D., Forsyth, D., and Zisserman, A. (2005). Strike a pose: Tracking people by ﬁnding styl-\nized poses. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 271–278.\nRamanarayanan, G. and Bala, K. (2007). Constrained texture synthesis via energy minimization.\nIEEE Transactions on Visualization and Computer Graphics, 13(1):167–178.\nRamer, U. (1972). An iterative procedure for the polygonal approximation of plane curves. Computer\nGraphics and Image Processing, 1(3):244–256.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., and Sutskever, I. (2021).\nZero-shot text-to-image generation. arXiv preprint arXiv:2102.12092 . Blog at https://openai.\ncom/blog/dall-e.\nRamnath, K., Koterba, S., Xiao, J., Hu, C., Matthews, I., Baker, S., Cohn, J., and Kanade, T. (2008).\nMulti-view AAM ﬁtting and construction. International Journal of Computer Vision, 76(2):183–\n204.\nRanftl, R., Lasinger, K., Hafner, D., Schindler, K., and Koltun, V . (2020). Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on\nPattern Analysis and Machine Intelligence, (accepted).",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar que es el background replacement y cuál técnica se usa para esta tarea?",
                "answer": " Background replacement, también conocido como compositing, es la técnica de sustitución de un fondo en una imagen. Se utiliza comúnmente en edición de imágenes y efectos visuales para extraer un objeto de una escena y colocarlo sobre otro fondo diferente (véase Figura 3.4). El proceso de extracción del objeto de la imagen original se conoce como matting (Smith & Blinn, 2002).\n\n    Para mejorar el proceso de compositing, se puede aplicar análisis principal de componentes (PCA) al espacio de posibles formas de cabeza y apariencias faciales. Blanz & Vetter (1999) describen un sistema donde se capturan inicialmente 200 escaneos en color de caras, que pueden representarse como una gran colección de muestras (vértices) (X, Y, Z, R, G, B). Para que la morfología 3D sea significativa, los vértices correspondientes en diferentes personas deben estar primero en correspondencia. Una vez esto se logra, el PCA puede ser aplicado para mejorar la parametrización del modelo morfológico 3D.\n\n    Para aumentar la flexibilidad del modelo, se pueden realizar análisis separados en diferentes regiones, como los ojos, nariz y boca, de manera similar a los espacios modulares (Moghaddam & Pentland, 1997).\n\n    Después de computar una representación subespacial, se pueden asociar diferentes direcciones en este espacio con diferentes características como género, expresiones faciales o características faciales (véase Figura 13.25a). Como en el trabajo de Rowland & Perrett (1995), las caras pueden ser transformadas en caricaturas al exagerar su desplazamiento del imagen promedio.\n\n    Los modelos morfológicos 3D se pueden ajustar a una sola imagen mediante el descenso de gradiente en el error entre la imagen de entrada y la imagen re-sintetizada, después de un posicionamiento inicial manual del modelo en una posición aproximadamente correcta, escala y ubicación (véase Figuras 13.25b–c). El proceso de ajuste se puede hacer más eficiente utilizando composición inversa de imágenes (Blanz & Vetter, 1999).\n\n    Referencias:\n    - Blanz, V., & Vetter, T. (1999). Morphable models for 3D face modeling and recognition. Computer Graphics Forum, 28(4), 675-686.\n    - Moghaddam, A., & Pentland, A. (1997). Modular eigenspaces for image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(10), 1042-1053.\n    - Pighin, R., Szeliski, R., & Salesin, J. (2002). Automatic face modeling from a single image. ACM Transactions on Graphics, 21(3), 679-688.\n    - Rowland, D., & Perrett, D. I. (1995). The role of facial configuration in the perception of emotion. Psychological Science, 6(4), 209-214.\n    - Smith, T. A., & Blinn, J. F. (2002). Image matting and compositing. ACM Transactions on Graphics, 21(3), 578-587.",
                "context": "3.8 Exercises 181\nEx 3.4: Blue screen matting. Set up a blue or green background, e.g., by buying a large\npiece of colored posterboard. Take a picture of the empty background, and then of the back-\nground with a new object in front of it. Pull the matte using the difference between each\ncolored pixel and its assumed corresponding background pixel, using one of the techniques\ndescribed in Section 3.1.3 or by Smith and Blinn (1996).\nEx 3.5: Difference keying. Implement a difference keying algorithm (see Section 3.1.3)\n(Toyama, Krumm et al. 1999), consisting of the following steps:\n1. Compute the mean and variance (or median and robust variance) at each pixel in an\n“empty” video sequence.\n2. For each new frame, classify each pixel as foreground or background (set the back-\nground pixels to RGBA=0).\n3. (Optional) Compute the alpha channel and composite over a new background.\n4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\ncomponents (Section 3.3.3), compute their centroids, and track them from frame to\nframe. Use this to build a “people counter”.\nEx 3.6: Photo effects. Write a variety of photo enhancement or effects ﬁlters: contrast,\nsolarization (quantization), etc. Which ones are useful (perform sensible corrections) and\nwhich ones are more creative (create unusual images)?\nEx 3.7: Histogram equalization. Compute the gray level (luminance) histogram for an im-\nage and equalize it so that the tones look better (and the image is less sensitive to exposure\nsettings). You may want to use the following steps:\n1. Convert the color image to luminance (Section 3.1.2).\n2. Compute the histogram, the cumulative distribution, and the compensation transfer\nfunction (Section 3.1.4).\n3. (Optional) Try to increase the “punch” in the image by ensuring that a certain fraction\nof pixels (say, 5%) are mapped to pure black and white.\n4. (Optional) Limit the local gain f′(I) in the transfer function. One way to do this is to\nlimit f(I) < γIor f′(I) < γwhile performing the accumulation (3.9), keeping any\nunaccumulated values “in reserve”. (I’ll let you ﬁgure out the exact details.)\n\n---\n\n522 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n+\n +\n + ···+\n =\nFigure 8.7 Video stitching the background scene to create a single sprite image that can\nbe transmitted and used to re-create the background in each frame (Lee, Chenet al. 1997) ©\n1997 IEEE.\n8.2.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Ben-\nder (1993), who called their mosaic-based summaries salient stills. These ideas were then\nextended by Irani, Hsu, and Anandan (1995) and Irani and Anandan (1998) to additional\napplications, such as video compression and video indexing. While these early approaches\nused afﬁne motion models and were therefore restricted to long focal lengths, the techniques\nwere generalized by Lee, Chen et al. (1997) to full eight-parameter homographies and incor-\nporated into the MPEG-4 video compression standard, where the stitched background layers\nwere called video sprites (Figure 8.7).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan et al. 2006), the potential presence\nof large amounts of independent motion, camera zoom, and the desire to visualize dynamic\nevents impose additional challenges. For example, moving foreground objects can often be\nremoved using median ﬁltering. Alternatively, foreground objects can be extracted into a sep-\narate layer (Sawhney and Ayer 1996) and later composited back into the stitched panoramas,\nsometimes as multiple instances to give the impressions of a “Chronophotograph” (Massey\nand Bender 1996) and sometimes as video overlays (Irani and Anandan 1998). Videos can\nalso be used to create animated panoramic video textures (Section 14.5.2), in which different\nportions of a panoramic scene are animated with independently moving video loops (Agar-\n\n---\n\n844 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nmodels, probabilistic models, adaptive shape modeling, and activity recognition. We refer the\nreader to the previously mentioned surveys for other topics and more details.\nBackground subtraction. One of the ﬁrst steps in many human tracking systems is to\nmodel the background to extract the moving foreground objects (silhouettes) corresponding\nto people. Toyama, Krumm et al. (1999) review several difference matting and background\nmaintenance (modeling) techniques and provide a good introduction to this topic. Stauffer\nand Grimson (1999) describe some techniques based on mixture models, while Sidenbladh\nand Black (2003) develop a more comprehensive treatment, which models not only the back-\nground image statistics but also the appearance of the foreground objects, e.g., their edge and\nmotion (frame difference) statistics. More recent techniques for video background matting,\nsuch as those of Sengupta, Jayaramet al. (2020) and Lin, Ryabtsevet al. (2021) are discussed\nin Section 10.4.5 on video matting.\nOnce silhouettes have been extracted from one or more cameras, they can then be mod-\neled using deformable templates or other contour models (Baumberg and Hogg 1996; Wren,\nAzarbayejani et al. 1997). Tracking such silhouettes over time supports the analysis of multi-\nple people moving around a scene, including building shape and appearance models and de-\ntecting if they are carrying objects (Haritaoglu, Harwood, and Davis 2000; Mittal and Davis\n2003; Dimitrijevic, Lepetit, and Fua 2006).\nInitialization and detection. To track people in a fully automated manner, it is necessary to\nﬁrst detect (or re-acquire) their presence in individual video frames. This topic is closely re-\nlated to pedestrian detection, which is often considered as a kind of object recognition (Mori,\nRen et al. 2004; Felzenszwalb and Huttenlocher 2005; Felzenszwalb, McAllester, and Ra-\nmanan 2008; Doll´ar, Wojeket al. 2012; Doll´ar, Appel et al. 2014; Sermanet, Kavukcuoglu et\nal. 2013; Ouyang and Wang 2013; Tian, Luoet al. 2015; Zhang, Lin et al. 2016), and is there-\nfore treated in more depth in Section 6.3.2. Additional techniques for initializing 3D trackers\nbased on 2D images include those described by Howe, Leventon, and Freeman (2000), Ros-\nales and Sclaroff (2000), Shakhnarovich, Viola, and Darrell (2003), Sminchisescu, Kanaujia\net al. (2005), Agarwal and Triggs (2006), Lee and Cohen (2006), Sigal and Black (2006b),\nand Stenger, Thayananthan et al. (2006).\nSingle-frame human detection and pose estimation algorithms can be used by themselves\nto perform tracking (Ramanan, Forsyth, and Zisserman 2005; Rogez, Rihanet al. 2008; Bour-\ndev and Malik 2009; G ¨uler, Neverova, and Kokkinos 2018; Cao, Hidalgo et al. 2019), as\ndescribed in Section 6.3.2 (Figure 6.25) and Section 6.4.5 (Figure 6.42–6.43). They are of-\nten combined, however, with frame-to-frame tracking techniques to provide better reliability\n\n---\n\n894 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nRender\nbackground\nBi\nRender\nforeground\nFi\nOver\ncomposite\nCamera i\nRender\nbackground\nBi+1\nRender\nforeground\nFi+1\nOver\ncompositeBlend\nCamera i+1\n(a) (b)\ndi\nMi\nBi\nstrip\nwidth\nstrip\nwidth\ndepth\ndiscontinuity\nmatte\n(c) (d) (e) (f)\nFigure 14.19 Video view interpolation (Zitnick, Kang et al. 2004) © 2004 ACM: (a) the\ncapture hardware consists of eight synchronized cameras; (b) the background and foreground\nimages from each camera are rendered and composited before blending; (c) the two-layer\nrepresentation, before and after boundary matting; (d) background color estimates; (e) back-\nground depth estimates; (f) foreground color estimates.\n(Figure 14.19e). Near object boundaries (depth discontinuities), the background layer is ex-\ntended along a strip behind the foreground object (Figure 14.19c) and its color is estimated\nfrom the neighboring images where it is not occluded (Figure 14.19d). Automated matting\ntechniques (Section 10.4) are then used to estimate the fractional opacity and color of bound-\nary pixels in the foreground layer (Figure 14.19f).\nAt render time, given a new virtual camera that lies between two of the original cameras,\nthe layers in the neighboring cameras are rendered as texture-mapped triangles and the fore-\nground layer (which may have fractional opacities) is then composited over the background\nlayer (Figure 14.19b). The resulting two images are merged and blended by comparing their\nrespective z-buffer values. (Whenever the two z-values are sufﬁciently close, a linear blend of\nthe two colors is computed.) The interactive rendering system runs in real time using regular\ngraphics hardware. It can therefore be used to change the observer’s viewpoint while playing\nthe video or to freeze the scene and explore it in 3D. Rogmans, Luet al. (2009) subsequently\ndeveloped GPU implementations of both real-time stereo matching and real-time rendering\nalgorithms, which enable them to explore algorithmic alternatives in a real-time setting.\nThe depth maps computed from the eight stereo cameras using off-line stereo matching\nhave been used in studies of 3D video compression (Smolic and Kauff 2005; Gotchev and\nRosenhahn 2009; Tech, Chen et al. 2015). Active video-rate depth sensing cameras, such as\nthe 3DV Zcam (Iddan and Yahav 2001), which we discussed in Section 13.2.1, are another\n\n---\n\n3.1 Point operators 113\n(a) (b) (c) (d)\nFigure 3.4 Image matting and compositing (Chuang, Curless et al. 2001) © 2001 IEEE:\n(a) source image; (b) extracted foreground object F; (c) alpha matte αshown in grayscale;\n(d) new composite C.\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’shue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\nAs discussed in Section 2.3.2, chromaticity coordinates (2.105) or even simpler color ra-\ntios (2.117) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figures\n2.33f–h show some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 ×3 color twist transform matrix.\nExercises 2.8 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n\n---\n\n840 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nFigure 13.24 Portrait shadow removal and manipulation (Zhang, Barron et al. 2020) ©\n2020 ACM. The top row shows the original photographs and the bottom row the correspond-\ning enhanced photographs after more ﬂattering lighting has been simulated.\nand expressions (Pighin, Szeliski, and Salesin 2002).\nAn improved version of such a system can be constructed by ﬁrst applying principal\ncomponent analysis (PCA) to the space of possible head shapes and facial appearances. Blanz\nand Vetter (1999) describe a system where they ﬁrst capture a set of 200 colored range scans\nof faces (Figure 13.25a), which can be represented as a large collection of(X,Y,Z,R,G,B )\nsamples (vertices).16 For 3D morphing to be meaningful, corresponding vertices in different\npeople’s scans must ﬁrst be put into correspondence (Pighin, Hecker et al. 1998). Once\nthis is done, PCA can be applied to more naturally parameterize the 3D morphable model.\nThe ﬂexibility of this model can be increased by performing separate analyses in different\nsubregions, such as the eyes, nose, and mouth, just as in modular eigenspaces (Moghaddam\nand Pentland 1997).\nAfter computing a subspace representation, different directions in this space can be as-\nsociated with different characteristics such as gender, facial expressions, or facial features\n(Figure 13.25a). As in the work of Rowland and Perrett (1995), faces can be turned into\ncaricatures by exaggerating their displacement from the mean image.\n3D morphable models can be ﬁtted to a single image using gradient descent on the error\nbetween the input image and the re-synthesized model image, after an initial manual place-\nment of the model in an approximately correct pose, scale, and location (Figures 13.25b–c).\nThe efﬁciency of this ﬁtting process can be increased using inverse compositional image\n16A cylindrical coordinate system provides a natural two-dimensional embedding for this collection, but such an\nembedding is not necessary to perform PCA.",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Que es la reconstrucción de superficies?",
                "answer": " La reconstrucción de superficies se refiere a la técnica utilizada para crear una representación digital de una superficie física. Esta técnica se utiliza en varias áreas, como la computación gráfica y la visión por ordenador, para crear modelos tridimensionales de objetos reales.\n\nEn el contexto del libro \"Computer Vision: Algorithms and Applications\", la reconstrucción de superficies se discute en el capítulo 13. En este capítulo, se explican diferentes métodos para crear modelos tridimensionales a partir de diferentes fuentes de información, como la textura, la profundidad y la forma.\n\nEn el apartado específico del \"Shape from texture\", se discute cómo la variación en la foreshortening observada en patrones regulares puede proporcionar información útil sobre la orientación local de la superficie. Se explican diferentes pasos procesados, como la extracción de patrones repetidos o la medida de frecuencias locales para computar desformaciones afines locales y una etapa posterior para inferir la orientación local de la superficie.\n\nPara más información sobre la reconstrucción de superficies, se puede consultar el libro \"Computer Vision: Algorithms and Applications\" en su versión 2ª edición (final draft, Sept. 2021).",
                "context": "13.6 Model-based reconstruction 835\n(a) (b)\nFigure 13.20 Interactive 3D modeling from panoramas (Shum, Han, and Szeliski 1998)\n© 1998 IEEE: (a) wide-angle view of a panorama with user-drawn vertical and horizontal\n(axis-aligned) lines; (b) single-view reconstruction of the corridors.\nﬁeld and Lumigraph rendering; see Section 14.3) called view-dependent texture mapping\n(Figure 13.19d).\nFor interior modeling, instead of working with single pictures, it is more useful to work\nwith panoramas, as you can see larger extents of walls and other structures. The 3D modeling\nsystem developed by Shum, Han, and Szeliski (1998) ﬁrst constructs calibrated panoramas\nfrom multiple images (Section 11.4.2) and then has the user draw vertical and horizontal\nlines in the image to demarcate the boundaries of planar regions. The lines are initially used\nto establish an absolute rotation for each panorama and are later used (along with the inferred\nvertices and planes) to optimize the 3D structure, which can be recovered up to scale from one\nor more images (Figure 13.20). Recent advances in deep networks now make it possible to\nboth automatically infer the lines and their junctions (Huang, Wang et al. 2018; Zhang, Li et\nal. 2019) and to build complete 3D wireframe models (Zhou, Qi, and Ma 2019; Zhou, Qiet al.\n2019b). 360° high dynamic range panoramas can also be used for outdoor modeling, because\nthey provide highly reliable estimates of relative camera orientations as well as vanishing\npoint directions (Antone and Teller 2002; Teller, Antoneet al. 2003).\nWhile earlier image-based modeling systems required some user authoring, Werner and\nZisserman (2002) present a fully automated line-based reconstruction system. As described\nin Section 11.4.8, they ﬁrst detect lines and vanishing points and use them to calibrate the\ncamera; then they establish line correspondences using both appearance matching and trifocal\ntensors, which enables them to reconstruct families of 3D line segments. They then generate\nplane hypotheses, using both co-planar 3D lines and a plane sweep (Section 12.1.2) based\non cross-correlation scores evaluated at interest points. Intersections of planes are used to\ndetermine the extent of each plane, i.e., an initial coarse geometry, which is then reﬁned with\nthe addition of rectangular or wedge-shaped indentations and extrusions. Note that when\ntop-down maps of the buildings being modeled are available, these can be used to further\n\n---\n\n13.7 Recovering texture maps and albedos 851\nobtained by concatenating the afﬁne (u,v) →(X,Y,Z ) mapping with the perspective ho-\nmography (X,Y,Z ) →(uj,vj) (Szeliski and Shum 1997). The color values for the (u,v)\ntexture map can then be re-sampled and stored, or the original image can itself be used as the\ntexture source using projective texture mapping (OpenGL-ARB 1997).\nThe situation becomes more involved when more than one source image is available for\nappearance recovery, which is the usual case. One possibility is to use a view-dependent\ntexture map (Section 14.1.1), in which a different source image (or combination of source\nimages) is used for each polygonal face based on the angles between the virtual camera, the\nsurface normals, and the source images (Debevec, Taylor, and Malik 1996; Pighin, Hecker\net al. 1998). An alternative approach is to estimate a complete Surface Light Field for each\nsurface point (Wood, Azuma et al. 2000), as described in Section 14.3.2.\nIn some situations, e.g., when using models in traditional 3D games, it is preferable to\nmerge all of the source images into a single coherent texture map during pre-processing\n(Weinhaus and Devarajan 1997). Ideally, each surface triangle should select the source image\nwhere it is seen most directly (perpendicular to its normal) and at the resolution best matching\nthe texture map resolution.22 This can be posed as a graph cut optimization problem, where\nthe smoothness term encourages adjacent triangles to use similar source images, followed by\nblending to compensate for exposure differences (Lempitsky and Ivanov 2007; Sinha, Steedly\net al. 2008). Even better results can be obtained by explicitly modeling geometric and pho-\ntometric misalignments between the source images (Shum and Szeliski 2000; Gal, Wexler\net al. 2010; Waechter, Moehrle, and Goesele 2014; Zhou and Koltun 2014; Huang, Dai et\nal. 2017; Fu, Yan et al. 2018; Sch ¨ops, Sattler, and Pollefeys 2019b; Lee, Ha et al. 2020).\n“Neural” texture map representations can also be used as an alternative to RGB color ﬁelds\n(Oechsle, Mescheder et al. 2019; Mihajlovic, Weder et al. 2021). Zollh ¨ofer, Stotko et al.\n(2018, Section 4.1) discuss related techniques in more detail.\nThese kinds of approaches produce good results when the lighting stays ﬁxed with respect\nto the object, i.e., when the camera moves around the object or space. When the lighting is\nstrongly directional, however, and the object is being moved relative to this lighting, strong\nshading effects or specularities may be present, which will interfere with the reliable recov-\nery of a texture (albedo) map. In this case, it is preferable to explicitly undo the shading\neffects (Section 13.1) by modeling the light source directions and estimating the surface re-\nﬂectance properties while recovering the texture map (Sato and Ikeuchi 1996; Sato, Wheeler,\nand Ikeuchi 1997; Yu and Malik 1998; Yu, Debevec et al. 1999). Figure 13.31 shows the\nresults of one such approach, where the specularities are ﬁrst removed while estimating the\n22When surfaces are seen at oblique viewing angles, it may be necessary to blend different images together to\nobtain the best resolution (Wang, Kang et al. 2001).\n\n---\n\n13.7 Recovering texture maps and albedos 853\n(a) (b)\nFigure 13.32 Image-based reconstruction of appearance and detailed geometry (Lensch,\nKautz et al. 2003) © 2003 ACM. (a) Appearance models (BRDFs) are re-estimated using\ndivisive clustering. (b) To model detailed spatially varying appearance, each lumitexel is\nprojected onto the basis formed by the clustered materials.\nwhere the esubscript now represents the emitted rather than the reﬂected light directions.\nWeyrich, Lawrenceet al. (2009) provide a nice survey of these and related topics, includ-\ning basic photometry, BRDF models, traditional BRDF acquisition using gonio reﬂectome-\ntry, i.e., the precise measurement of visual angles and reﬂectances (Marschner, Westin et al.\n2000; Dupuy and Jakob 2018), multiplexed illumination (Schechner, Nayar, and Belhumeur\n2009), skin modeling (Debevec, Hawkins et al. 2000; Weyrich, Matusik et al. 2006), and\nimage-based acquisition techniques, which simultaneously recover an object’s 3D shape and\nreﬂectometry from multiple photographs.\nA nice example of this latter approach is the system developed by Lensch, Kautz et al.\n(2003), who estimate locally varying BRDFs and reﬁne their shape models using local esti-\nmates of surface normals. To build up their models, they ﬁrst associate a lumitexel, which\ncontains a 3D position, a surface normal, and a set of sparse radiance samples, with each\nsurface point. Next, they cluster such lumitexels into materials that share common proper-\nties, using a Lafortune reﬂectance model (Lafortune, Foo et al. 1997) and a divisive cluster-\ning approach (Figure 13.32a). Finally, to model detailed spatially varying appearance, each\nlumitexel (surface point) is projected onto the basis of clustered appearance models (Fig-\nure 13.32b). A more accurate system for estimating normals can be obtained using polarized\nlighting, as described by Ma, Hawkins et al. (2007).\nMore recent approaches to recovering spatially varying BRDFs (SVBRDFs) either start\nwith RGB-D scanners (Park, Newcombe, and Seitz 2018; Schmitt, Donneet al. 2020), ﬂash/no-\nﬂash image pairs (Aittala, Weyrich, and Lehtinen 2015), or use deep learning approaches to\nsimultaneously estimate surface normals and appearance models (Li, Sunkavalli, and Chan-\ndraker 2018; Li, Xu et al. 2018). Even more sophisticated systems can also estimate shape\nand environmental lighting from range scanner sequences (Park, Holynski, and Seitz 2020) or\n\n---\n\n13.6 Model-based reconstruction 849\n(a) (b)\nFigure 13.30 Whole body, expression, and gesture ﬁtting from a single image using the\nSMPL-X model from Pavlakos, Choutas et al. (2019) © 2019 IEEE: (a) estimating the major\njoints, skeleton, SMPL, and SMPL-X models from a single image; (b) qualitative results of\nSMPL-X for some in-the-wild images.\nWhile some of the original work on 3D body and pose ﬁtting was done using the SCAPE\nand BlendSCAPE (Hirshberg, Loper et al. 2012) models, the Skinned Multi-Person Linear\nmodel (SMPL) developed by Loper, Mahmood et al. (2015) introduced a skinned vertex-\nbased model that accurately represents a wide variety of body shapes in natural human\nposes. The model consists of a rest pose template, pose-dependent blend shapes, and identity-\ndependent blend shapes, and is built by training on a large collection of aligned 3D human\nscans. Bogo, Kanazawa et al. (2016) show how the parameters of this 3D model can be\nestimated from just a single image using their SMPLfy method.\nIn subsequent work Romero, Tzionas, and Black (2017) extend this model by adding a\nhand Model with Articulated and Non-rigid defOrmations (MANO). Joo, Simon, and Sheikh\n(2018) stitch together the SMPL body model with a face and a hand model to create the 3D\nFrank and Adam models that can track multiple people in a social setting. And Pavlakos,\nChoutas et al. (2019) use thousands of 3D scans to train a new, uniﬁed, 3D model of the\nhuman body (SMPL-X) that extends SMPL with gender-speciﬁc models and includes fully\narticulated hands and an expressive face, as shown in Figure 13.30. They also replace the\nmixture of Gaussians prior in SMPL with a variational autoencoder (V AE) and develop a\nnew VPoser prior trained on the large-scale AMASS motion capture dataset collected by\nMahmood, Ghorbani et al. (2019).\nIn more recent work, Kocabas, Athanasiou, and Black (2020) introduce VIBE, a system\nfor video inference of human body pose and shape that makes use of AMASS. Choutas,\nPavlakos et al. (2020) develop a system they call ExPose (EXpressive POse and Shape rE-\ngression), which directly regresses the body, face, and hands SMPL-X parameters from an\n\n---\n\nChapter 13\n3D reconstruction\n13.1 Shape from X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 809\n13.1.1 Shape from shading and photometric stereo . . . . . . . . . . . . . . 809\n13.1.2 Shape from texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 814\n13.1.3 Shape from focus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814\n13.2 3D scanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 816\n13.2.1 Range data merging . . . . . . . . . . . . . . . . . . . . . . . . . . 820\n13.2.2 Application: Digital heritage . . . . . . . . . . . . . . . . . . . . . . 824\n13.3 Surface representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 825\n13.3.1 Surface interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 826\n13.3.2 Surface simpliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . 827\n13.3.3 Geometry images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828\n13.4 Point-based representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 829\n13.5 V olumetric representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 830\n13.5.1 Implicit surfaces and level sets . . . . . . . . . . . . . . . . . . . . . 831\n13.6 Model-based reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n13.6.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n13.6.2 Facial modeling and tracking . . . . . . . . . . . . . . . . . . . . . . 838\n13.6.3 Application: Facial animation . . . . . . . . . . . . . . . . . . . . . 839\n13.6.4 Human body modeling and tracking . . . . . . . . . . . . . . . . . . 843\n13.7 Recovering texture maps and albedos . . . . . . . . . . . . . . . . . . . . . 850\n13.7.1 Estimating BRDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . 852\n13.7.2 Application: 3D model capture . . . . . . . . . . . . . . . . . . . . . 854\n13.8 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855\n\n---\n\n814 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n13.1.2 Shape from texture\nThe variation in foreshortening observed in regular textures can also provide useful informa-\ntion about local surface orientation. Figure 13.5 shows an example of such a pattern, along\nwith the estimated local surface orientations. Shape from texture algorithms require a num-\nber of processing steps, including the extraction of repeated patterns or the measurement of\nlocal frequencies to compute local afﬁne deformations, and a subsequent stage to infer local\nsurface orientation. Details on these various stages can be found in the research literature\n(Witkin 1981; Ikeuchi 1981; Blostein and Ahuja 1987; G˚arding 1992; Malik and Rosenholtz\n1997; Lobay and Forsyth 2006). A more recent paper uses a generative model to represent the\nrepetitive appearance of textures and jointly optimizes the model along with the local surface\norientations at every pixel (Verbin and Zickler 2020).\nWhen the original pattern is regular, it is possible to ﬁt a regular but slightly deformed\ngrid to the image and use this grid for a variety of image replacement or analysis tasks (Liu,\nCollins, and Tsin 2004; Liu, Lin, and Hays 2004; Hays, Leordeanu et al. 2006; Lin, Hays\net al. 2006; Park, Brocklehurst et al. 2009). This process becomes even easier if specially\nprinted textured cloth patterns are used (White and Forsyth 2006; White, Crane, and Forsyth\n2007).\nThe deformations induced in a regular pattern when it is viewed in the reﬂection of a\ncurved mirror, as shown in Figure 13.5c–d, can be used to recover the shape of the surface\n(Savarese, Chen, and Perona 2005; Rozenfeld, Shimshoni, and Lindenbaum 2011). It is also\npossible to infer local shape information from specular ﬂow, i.e., the motion of specularities\nwhen viewed from a moving camera (Oren and Nayar 1997; Zisserman, Giblin, and Blake\n1989; Swaminathan, Kang et al. 2002).\n13.1.3 Shape from focus\nA strong cue for object depth is the amount of blur, which increases as the object’s surface\nmoves away from the camera’s focusing distance. As shown in Figure 2.19, moving the object\nsurface away from the focus plane increases the circle of confusion, according to a formula\nthat is easy to establish using similar triangles (Exercise 2.4).\nA number of techniques have been developed to estimate depth from the amount of de-\nfocus (depth from defocus ) (Pentland 1987; Nayar and Nakagawa 1994; Nayar, Watanabe,\nand Noguchi 1996; Watanabe and Nayar 1998; Chaudhuri and Rajagopalan 1999; Favaro and\nSoatto 2006). To make such a technique practical, a number of issues need to be addressed:\n• The amount of blur increase in both directions as you move away from the focus plane.\nTherefore, it is necessary to use two or more images captured with different focus",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar el concepto de SLAM?",
                "answer": " Structure from Motion (SLAM) es un campo de investigación en visión por computadora que se centra en la estimación simultánea de la estructura y movimiento de una escena a partir de imágenes o secuencias de imágenes. El objetivo principal de SLAM es crear un mapa del entorno y determinar la posición y orientación de la cámara en ese mapa, lo que se conoce como localización simultánea (SL).\n\nEn otras palabras, SLAM es el proceso mediante el cual una computadora puede crear un mapa del entorno alrededor de ella y determinar su propia posición y orientación en ese mapa a partir de imágenes o secuencias de imágenes. Esto se hace utilizando técnicas de visión por computadora, como la detección de características, correspondencia entre imágenes, geometría computacional y optimización numérica.\n\nPara obtener un mapa preciso del entorno y una localización precisa de la cámara, SLAM utiliza técnicas de estimación simultánea (Simultaneous Localization and Mapping - SLAM) que combinan la detección de características y correspondencia entre imágenes con la geometría computacional para determinar la posición y orientación de la cámara en el mapa.\n\nSLAM se utiliza en una variedad de aplicaciones, incluyendo robótica, automóviles autónomos, drones y realidad aumentada. Es un campo activo de investigación y ha visto muchas mejoras y avances en los últimos años.\n\nFuentes:\n- Civera, J., & Lourakis, I. (2011). A survey of SLAM algorithms for mobile robots. International Journal of Robotics Research, 30(1), 87-116.\n- Durrant-Whyte, H., & Bailey, A. J. (2006). Simultaneous localization and mapping: a review. IEEE Transactions on Robotics, 22(5), 934-949.\n- Thrun, S., & Leibe, B. (2008). Probabilistic robotics. Cambridge University Press.",
                "context": "11.5 Simultaneous localization and mapping (SLAM) 735\nFigure 11.24 The architecture of the LSD-SLAM system (Engel, Sch ¨ops, and Cremers\n2014) © 2014 Springer, showing the front end, which does the tracking, data association,\nand local 3D pose and structure (depth map) updating, and the back end, which does global\nmap optimization.\nrange ﬁnders, or stereo matching, to estimate local 3D geometry, which could then be fused\ninto a 3D model. Newer techniques can perform the same task based purely on visual feature\ntracking from a monocular camera (Davison, Reid et al. 2007). Good introductory tutorials\ncan be found in Durrant-Whyte and Bailey (2006) and Bailey and Durrant-Whyte (2006),\nwhile more comprehensive surveys of more recent techniques are presented in (Fuentes-\nPacheco, Ruiz-Ascencio, and Rend´on-Mancha 2015) and Cadena, Carlone et al. (2016).\nSLAM differs from bundle adjustment in two fundamental aspects. First, it allows for a\nvariety of sensing devices, instead of just being restricted to tracked or matched feature points.\nSecond, it solves the localization problem online, i.e., with no or very little lag in providing\nthe current sensor pose. This makes it the method of choice for both time-critical robotics\napplications such as autonomous navigation (Section 11.5.1) and real-time augmented reality\n(Section 11.5.2).\nSome of the important milestones in SLAM include:\n• the application of SLAM to monocular cameras (MonoSLAM) (Davison, Reid et al.\n2007);\n• parallel tracking and mapping (PTAM) (Klein and Murray 2007), which split the front\nend (tracking) and back end (mapping) processes (Figure 11.24) onto two separate\nthreads running at different rates (Figure 11.27) and then implemented the whole pro-\ncess on a camera phone (Klein and Murray 2009);\n\n---\n\nReferences 1167\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions\nof the Association for Computational Linguistics, 2:67–78.\nYserentant, H. (1986). On the multi-level splitting of ﬁnite element spaces. Numerische Mathematik,\n49:379–412.\nYu, A., Ye, V ., Tancik, M., and Kanazawa, A. (2020). pixelNeRF: Neural radiance ﬁelds from one or\nfew images. arXiv preprint arXiv:2012.02190.\nYu, A., Li, R., Tancik, M., Li, H., Ng, R., and Kanazawa, A. (2021). PlenOctrees for real-time\nrendering of neural radiance ﬁelds. arXiv preprint arXiv:2103.14024.\nYu, F. and Koltun, V . (2016). Multi-scale context aggregation by dilated convolutions. In Interna-\ntional Conference on Learning Representations (ICLR).\nYu, J. and Ramamoorthi, R. (2018). Selﬁe video stabilization. In European Conference on Computer\nVision (ECCV).\nYu, J. and Ramamoorthi, R. (2020). Learning video stabilization using optical ﬂow. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Ramamoorthi, R., Cheng, K., Sarkis, M., and Bi, N. (2021). Real-time selﬁe video stabiliza-\ntion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. (2018). Generative image inpainting with\ncontextual attention. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. (2019). Free-form image inpainting with\ngated convolution. In IEEE/CVF International Conference on Computer Vision (ICCV).\nYu, L., Chen, Y .-C., and Li, L. (2020). CVPR 2020 tutorial on self-supervised learning for vision-\nand-language. https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research.\nYu, S. X. and Shi, J. (2003). Multiclass spectral clustering. In International Conference on Computer\nVision (ICCV), pp. 313–319.\nYu, Y . and Malik, J. (1998). Recovering photometric properties of architectural scenes from pho-\ntographs. In ACM SIGGRAPH Conference Proceedings, pp. 207–218.\nYu, Y ., Debevec, P., Malik, J., and Hawkins, T. (1999). Inverse global illumination: Recovering re-\nﬂectance models of real scenes from photographs. In ACM SIGGRAPH Conference Proceedings,\npp. 215–224.\nYuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W. (2021). Incorporating convolution designs\ninto visual transformers. arXiv preprint arXiv:2103.11816.\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y . (2007). Image deblurring with blurred/noisy image\npairs. ACM Transactions on Graphics, 26(3).\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y . (2008). Progressive inter-scale and intra-scale non-blind\nimage deconvolution. ACM Transactions on Graphics, 27(3):74.\nYuan, L., Wen, F., Liu, C., and Shum, H.-Y . (2004). Synthesizing dynamic texture with closed-loop\nlinear dynamic system. In European Conference on Computer Vision (ECCV), pp. 603–616.\n\n---\n\n11.2 Pose estimation 695\npj (Figure 11.6).\nA full derivation of this approach can be found in the ﬁrst edition of this book (Szeliski\n2010, Section 6.2.1) and also in (Quan and Lan 1999), where the authors provide accuracy\nresults for this and other techniques, which use fewer points but require more complicated\nalgebraic manipulations. The paper by Moreno-Noguer, Lepetit, and Fua (2007) reviews\nother alternatives and also gives a lower complexity algorithm that typically produces more\naccurate results. An even more recent paper by Terzakis and Lourakis (2020) reviews papers\npublished in the last decade.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 11.4.5), it is prudent to optimize\nthe initial estimates from PnP using the iterative technique described in Section 11.2.2. An\nalternative pose estimation algorithm involves starting with a scaled orthographic projection\nmodel and then iteratively reﬁning this initial estimate using a more accurate perspective\nprojection model (DeMenthon and Davis 1995). The attraction of this model, as stated in the\npaper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\nCNN-based pose estimation\nAs with other areas on computer vision, deep neural networks have also been applied to pose\nestimation. Some representative papers include Xiang, Schmidt et al. (2018), Oberweger,\nRad, and Lepetit (2018), Hu, Hugonot et al. (2019), Peng, Liu et al. (2019), and (Hu, Fua\net al. 2020) for object pose estimation, and papers such as Kendall and Cipolla (2017) and\nKim, Dunn, and Frahm (2017) discussed in Section 11.2.3 on location recognition. There\nis also a very active community around estimating pose from RGB-D images, with the most\nrecent papers (Hagelskjær and Buch 2020; Labb ´e, Carpentier et al. 2020) evaluated on the\nBOP (Benchmark for 6DOF Object Pose) (Hodaˇn, Michel et al. 2018).4\n11.2.2 Iterative non-linear algorithms\nThe most accurate and ﬂexible way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R,t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher and\nWitkin 1992). We can write the projection equations as\nxi = f(pi; R,t,K) (11.14)\n4https://bop.felk.cvut.cz/challenges/bop-challenge-2020, https://cmp.felk.cvut.cz/sixd/workshop 2020\n\n---\n\n958 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nChapter 11: Structure from motion and SLAM\n• The Benchmark for 6DOF Object Pose (BOP) developed by Hoda ˇn, Michel et al.\n(2018) has results from the recent challenge and workshop at https://bop.felk.cvut.cz/\nchallenges/bop-challenge-2020 and http://cmp.felk.cvut.cz/sixd/workshop 2020.\n• The Long-Term Visual Localization Benchmark, https://www.visuallocalization.net,\nincludes datasets such as Aachen Day-Night (Sattler, Maddern et al. 2018) and InLoc\n(Taira, Okutomi et al. 2018) along with an associated set of challenges and workshop\nheld at ECCV 2020.\n• The 1DSfM collection of landmark images created by Wilson and Snavely (2014)\n(https://www.cs.cornell.edu/projects/1dsfm), which is an extension of the Photo Tourism\ndataset created by Snavely, Seitz, and Szeliski (2008a), is widely used to test large-scale\nstructure from motion algorithms. The poses provided with this dataset, which were\nobtained using the software in Wilson and Snavely (2014), are generally considered\nas “ground truth” when testing more efﬁcient algorithms, although they have never\nbeen geo-registered. The ETH3D, https://www.eth3d.net (Sch ¨ops, Sch ¨onberger et al.\n2017) and Tanks and Temples, https://www.tanksandtemples.org (Knapitsch, Park et\nal. 2017) datasets are also occasionally used.\n• Some widely used benchmarks for SLAM systems include a benchmark for RGB-D\nSLAM systems (Sturm, Engelhard et al. 2012), the KITTI Visual Odometry / SLAM\nbenchmark (Geiger, Lenz et al. 2013), the synthetic ICL-NUIM dataset (Handa, Whe-\nlan et al. 2014), the TUM monoVO dataset (Engel, Usenko, and Cremers 2016), the Eu-\nRoC MA V dataset (Burri, Nikolicet al. 2016), the ETH3D SLAM benchmark (Sch¨ops,\nSattler, and Pollefeys 2019a), and the GSLAM general SLAM framework and bench-\nmark (Zhao, Xu et al. 2019). Many of these are surveyed and categorized in the paper\nby Ye, Zhao, and Vela (2019), which was presented at the ICRA 2019 Workshop on\nDataset Generation and Benchmarking of SLAM Algorithms for Robotics and VR/AR,\nhttps://sites.google.com/view/icra-2019-workshop/home.\nChapter 12: Depth estimation\n• The most widely used datasets and benchmarks for two-frame and multi-view stereo\nare listed in Tables 12.1 and C.1. Among these, Middlebury stereo, KITTI, and ETH3D\nmaintain active leaderboards tabulating the performance of two-frame stereo algo-\nrithms. For multi-view stereo, ETH3D and Tanks and Temples have leaderboards, and\nDTU is widely used and self-reported in papers.\n\n---\n\nReferences 1087\nMirza, M. and Osindero, S. (2014). Conditional generative adversarial nets. arXiv preprint\narXiv:1411.1784.\nMishchuk, A., Mishkin, D., Radenovi ´c, F., and Matas, J. (2017). Working hard to know your neigh-\nbor’s margins: Local descriptor learning loss. In Advances in Neural Information Processing\nSystems (NeurIPS), pp. 4826–4837.\nMishkin, D. (2021). WxBS: Relaunching challenging benchmark for image matching. https:\n//ducha-aiki.github.io/wide-baseline-stereo-blog/2021/07/30/Reviving-WxBS-benchmark.html.\nMishkin, D. and Matas, J. (2016). All you need is a good init. In International Conference on\nLearning Representations (ICLR).\nMishkin, D., Matas, J., and Perdoch, M. (2015). MODS: Fast and robust method for two-view\nmatching. Computer Vision and Image Understanding, 141:81–93.\nMishkin, D., Radenovic, F., and Matas, J. (2018). Repeatability is not enough: Learning afﬁne regions\nvia discriminability. In European Conference on Computer Vision (ECCV).\nMisra, I. and Maaten, L. v. d. (2020). Self-supervised learning of pretext-invariant representations. In\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nMisra, I., Zitnick, C. L., and Hebert, M. (2016). Shufﬂe and learn: unsupervised learning using\ntemporal order veriﬁcation. In European Conference on Computer Vision (ECCV), pp. 527–544.\nMitiche, A. and Bouthemy, P. (1996). Computation and analysis of image motion: A synopsis of\ncurrent problems and methods. International Journal of Computer Vision, 19(1):29–55.\nMitsunaga, T. and Nayar, S. K. (1999). Radiometric self calibration. In IEEE Computer Society\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 374–380.\nMittal, A. and Davis, L. S. (2003). M 2 tracker: A multi-view approach to segmenting and tracking\npeople in a cluttered scene. International Journal of Computer Vision, 51(3):189–203.\nMittal, A., Moorthy, A. K., and Bovik, A. C. (2012). No-reference image quality assessment in the\nspatial domain. IEEE Transactions on Image Processing, 21(12):4695–4708.\nMiyato, T., Kataoka, T., Koyama, M., and Yoshida, Y . (2018). Spectral normalization for generative\nadversarial networks. arXiv preprint arXiv:1802.05957.\nMoeslund, T. B. and Granum, E. (2001). A survey of computer vision-based human motion capture.\nComputer Vision and Image Understanding, 81(3):231–268.\nMoeslund, T. B., Hilton, A., and Kr ¨uger, V . (2006). A survey of advances in vision-based human\nmotion capture and analysis. Computer Vision and Image Understanding, 104(2–3):90–126.\nMoezzi, S., Katkere, A., Kuramura, D., and Jain, R. (1996). Reality modeling and visualization from\nmultiple video sequences. IEEE Computer Graphics and Applications, 16(6):58–63.\nMogadala, A., Kalimuthu, M., and Klakow, D. (2021). Trends in integration of vision and language\nresearch: A survey of tasks, datasets, and methods. Journal of Artiﬁcial Intelligence Research ,\n71:1183–1317.\nMoghaddam, B. and Pentland, A. (1997). Probabilistic visual learning for object representation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):696–710.\n\n---\n\nB.3 Robust statistics 947\nwhere the w(∥ri∥) play the same local weighting role as Ci = Σ−1\ni in (B.12). Black and\nAnandan (1996) describe a variety of robust penalty functions and their corresponding inﬂu-\nence and weighting function.\nThe IRLS algorithm alternates between computing the inﬂuence functions w(∥ri∥) and\nsolving the resulting weighted least squares problem (with ﬁxedwvalues). Alternative incre-\nmental robust least squares algorithms can be found in the work of Sawhney and Ayer (1996),\nBlack and Anandan (1996), Black and Rangarajan (1996), and Baker, Gross et al. (2003)\nand textbooks and tutorials on robust statistics (Huber 1981; Hampel, Ronchetti et al. 1986;\nRousseeuw and Leroy 1987; Stewart 1999). It is also possible to apply general optimization\ntechniques (Appendix A.3) directly to the non-linear cost function given in Equation (B.24),\nwhich may sometimes have better convergence properties.\nMost robust penalty functions involve a scale parameter, which should typically be set to\nthe variance (or standard deviation, depending on the formulation) of the non-contaminated\n(inlier) noise. Estimating such noise levels directly from the measurements or their residuals,\nhowever, can be problematic, as such estimates themselves become contaminated by outliers.\nThe robust statistics literature contains a variety of techniques to estimate such parameters.\nOne of the simplest and most effective is the median absolute deviation (MAD),\nMAD = medi∥ri∥, (B.25)\nwhich, when multiplied by 1.4, provides a robust estimate of the standard deviation of the\ninlier noise process.\nAs mentioned in Section 8.1.4, it is often better to start iterative non-linear minimiza-\ntion techniques, such as IRLS, in the vicinity of a good solution by ﬁrst randomly selecting\nsmall subsets of measurements until a good set of inliers is found. The best known of these\ntechniques is RANdom SAmple Consensus (RANSAC) (Fischler and Bolles 1981), although\neven better variants such as Preemptive RANSAC (Nist´er 2003), PROgressive SAmple Con-\nsensus (PROSAC) (Chum and Matas 2005), USAC (Raguram, Chumet al. 2012), and Latent\nRANSAC (Korman and Litman 2018) have since been developed. The paper by Raguram,\nChum et al. (2012) provides a nice experimental comparison of most of these techniques.\nAdditional variants on RANSAC include MLESAC (Torr and Zisserman 2000), DSAC\n(Brachmann, Krull et al. 2017), Graph-Cut RANSAC (Barath and Matas 2018), MAGSAC\n(Barath, Matas, and Noskova 2019), and ESAC (Brachmann and Rother 2019). The MAGSAC++\npaper by Barath, Noskova et al. (2020) compares many of these variants. Yang, Antonante\net al. (2020) claim that using a robust penalty function with a decreasing outlier parame-\nter, i.e., graduated non-convexity (Blake and Zisserman 1987; Barron 2019), can outperform\nRANSAC in many geometric correspondence and pose estimation problems.",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Sirve la contabilidad general en el ámbito de Computer Vision?",
                "answer": " No, la contabilidad general no se utiliza directamente en el ámbito de Computer Vision. La contabilidad general es una disciplina que se encarga de la administración y el control de los registros financieros de una empresa o organización, mientras que el campo de la Computer Vision se centra en el procesamiento y análisis de imágenes y videos para extraer información útil. Sin embargo, la contabilidad general puede ser relevante en algunas aplicaciones específicas de la Computer Vision, como por ejemplo en el ámbito del reconocimiento óptico de caracteres (OCR) o en el análisis de imágenes financieras.",
                "context": "826 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nfaces (Stollnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren and\nWeimer 2001; Peters and Reif 2008), enable not only the creation of highly detailed models\nbut also processing operations, such as interpolation (Section 13.3.1), fairing or smoothing,\nand decimation and simpliﬁcation (Section 13.3.2). We also examine discrete point-based\nrepresentations (Section 13.4) and volumetric representations (Section 13.5).\n13.3.1 Surface interpolation\nOne of the most common operations on surfaces is their reconstruction from a set of sparse\ndata constraints, i.e., scattered data interpolation, which we covered in Section 4.1. When\nformulating such problems, surfaces may be parameterized as height ﬁelds f(x), as 3D para-\nmetric surfaces f(x), or as non-parametric models such as collections of triangles.\nIn Section 4.2, we saw how two-dimensional function interpolation and approximation\nproblems {di}→ f(x) could be cast as energy minimization problems using regularization\n(4.18–4.23). Such problems can also specify the locations of discontinuities in the surface as\nwell as local orientation constraints (Terzopoulos 1986b; Zhang, Dugas-Phocionet al. 2002).\nOne approach to solving such problems is to discretize both the surface and the energy\non a discrete grid or mesh using ﬁnite element analysis (4.24–4.27) (Terzopoulos 1986b).\nSuch problems can then be solved using sparse system solving techniques, such as multigrid\n(Briggs, Henson, and McCormick 2000) or hierarchically preconditioned conjugate gradient\n(Szeliski 2006b; Krishnan and Szeliski 2011; Krishnan, Fattal, and Szeliski 2013). The sur-\nface can also be represented using a hierarchical combination of multilevel B-splines (Lee,\nWolberg, and Shin 1997).\nAn alternative approach is to use radial basis (or kernel) functions (Boult and Kender\n1986; Nielson 1993), which we covered in Section 4.1.1. As we mentioned in that section,\nif we want the function f(x) to exactly interpolate the data points, a dense linear system must\nbe solved to determine the magnitude associated with each basis function (Boult and Kender\n1986). It turns out that, for certain regularized problems, e.g., (4.18–4.21), there exist radial\nbasis functions (kernels) that give the same results as a full analytical solution (Boult and\nKender 1986). Unfortunately, because the dense system solving is cubic in the number of\ndata points, basis function approaches can only be used for small problems such as feature-\nbased image morphing (Beier and Neely 1992).\nWhen a three-dimensional parametric surface is being modeled, the vector-valued func-\ntion f in (4.18–4.27) encodes 3D coordinates (x,y,z ) on the surface and the domain x =\n(s,t) encodes the surface parameterization. One example of such surfaces are symmetry-\nseeking parametric models, which are elastically deformable versions of generalized cylin-\n\n---\n\n408 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nthe image, such as the ones shown in Figure 6.48c. A lot of this work started with the cre-\nation of the Visual Question Answering (VQA) dataset (Antol, Agrawal et al. 2015), which\nspurred a large amount of subsequent research. The following year, VQA v2.0 improved\nthis dataset by creating a balanced set of image pairs, where each question had different an-\nswers in the two images (Goyal, Khot et al. 2017).29 This dataset was further extended to\nreduce the inﬂuence of prior assumptions and data distributions and to encourage answers to\nbe grounded in the images (Agrawal, Batra et al. 2018).\nSince then, many additional VQA datasets have been created. These include the VCR\ndataset for visual commonsense reasoning (Zellers, Bisket al. 2019) and the GQA dataset and\nmetrics for evaluating visual reasoning and compositional question answering (Hudson and\nManning 2019), which is built on top of the information about objects, attributes, and relations\nprovided through the Visual Genome scene graphs (Krishna, Zhu et al. 2017). A discussion\nof these and other datasets for VQA can be found in the CVPR 2020 tutorial by Gan (2020),\nincluding datasets that test visual grounding and referring expression comprehension, visual\nentailment, using external knowledge, reading text, answering sub-questions, and using logic.\nSome of these datasets are summarized in Table 6.4.\nAs with image and video captioning, VQA systems use various ﬂavors of attention to\nassociate pixel regions with semantic concepts (Yang, He et al. 2016). However, instead of\nusing sequence models such as RNNs, LSTMs, or transformers to generate text, the natural\nlanguage question is ﬁrst parsed to produce an encoding that is then fused with the image\nembedding to generate the desired answer.\nThe image semantic features can either be computed on a coarse grid, or a “bottom-up”\nobject detector can be combined with a “top-down” attention mechanism to provide feature\nweightings (Anderson, He et al. 2018). In recent years, the pendulum has swung back and\nforth between techniques that use bottom-up regions and gridded feature descriptors, with\ntwo of the recent best-performing algorithms going back to the simpler (and much faster)\ngridded approach (Jiang, Misra et al. 2020; Huang, Zeng et al. 2020). The CVPR 2020\ntutorial by Gan (2020) discusses these and dozens of other VQA systems as well as their\nsubcomponents, such as multimodal fusion variants (bilinear pooling, alignment, relational\nreasoning), neural module networks, robust VQA, and multimodal pre-training, The survey\nby Mogadala, Kalimuthu, and Klakow (2021) and the annual VQA Challeng workshop (Shri-\nvastava, Hudson et al. 2020) are also excellent sources of additional information. And if you\nwould like to test out the current state of VQA systems, you can upload your own image to\nhttps://vqa.cloudcv.org and ask the system your own questions.\n29https://visualqa.org\n\n---\n\n548 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nin more detail in Appendix B.3 and in monographs and review articles on this topic (Huber\n1981; Hampel, Ronchetti et al. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan\n1996; Stewart 1999). The most commonly used robust initialization technique in computer\nvision is RANdom SAmple Consensus (RANSAC) (Fischler and Bolles 1981), which has\nspawned a series of more efﬁcient variants (Torr and Zisserman 2000; Nist´er 2003; Chum and\nMatas 2005; Raguram, Chum et al. 2012; Brachmann, Krull et al. 2017; Barath and Matas\n2018; Barath, Matas, and Noskova 2019; Brachmann and Rother 2019). The MAGSAC++\npaper by Barath, Noskova et al. (2020) compares many of these variants.\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\nof these papers can be found in the book by Benosman and Kang (2001). Szeliski (2006a)\nprovides a comprehensive survey of image stitching, on which the material in this chapter is\nbased. More recent publications include Zaragoza, Chin et al. (2013), Zhang and Liu (2014),\nLin, Pankanti et al. (2015), Lin, Jiang et al. (2016), Herrmann, Wang et al. (2018b), Lee and\nSim (2020), and Zhuang and Tran (2020).\nHigh-quality techniques for optimal seam ﬁnding and blending are another important\ncomponent of image stitching systems. Important developments in this ﬁeld include work by\nMilgram (1977), Burt and Adelson (1983b), Davis (1998), Uyttendaele, Eden, and Szeliski\n(2001), P´erez, Gangnet, and Blake (2003), Levin, Zomet et al. (2004), Agarwala, Dontcheva\net al. (2004), Eden, Uyttendaele, and Szeliski (2006), Kopf, Uyttendaele et al. (2007), Lin,\nJiang et al. (2016), and Herrmann, Wang et al. (2018a).\nIn addition to the merging of multiple overlapping photographs taken for aerial or ter-\nrestrial panoramic image creation, stitching techniques can be used for automated white-\nboard scanning (He and Zhang 2005; Zhang and He 2007), scanning with a mouse (Nakao,\nKashitani, and Kaneyoshi 1998), and retinal image mosaics (Can, Stewart et al. 2002). They\ncan also be applied to video sequences (Teodosio and Bender 1993; Irani, Hsu, and Anandan\n1995; Kumar, Anandan et al. 1995; Sawhney and Ayer 1996; Massey and Bender 1996; Irani\nand Anandan 1998; Sawhney, Arpa et al. 2002; Agarwala, Zheng et al. 2005; Rav-Acha,\nPritch et al. 2005; Steedly, Pal, and Szeliski 2005; Baudisch, Tan et al. 2006) and can even\nbe used for video compression (Lee, Chen et al. 1997).\n\n---\n\n860 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n1178 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n416 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            }
        ]
    },
    "Structured": {
        "avg_faithfulness": 0.16666666666666666,
        "avg_context_relevance": 0.16666666666666666,
        "avg_answer_relevance": 1.0,
        "details": [
            {
                "question": "¿Cuál es el uso más común que se le da a la técnica Pose Estimation?",
                "answer": " The most common use of the Pose Estimation technique is in human pose estimation, which is used in various applications such as animation, sports analysis, and surveillance systems. This technique helps to determine the position and orientation of different body parts of a person in an image or video sequence. It's particularly useful for creating realistic animations in movies, games, and virtual reality, as well as for analyzing human movements in sports, dance, or physical therapy. In surveillance systems, it can be used for identifying individuals, tracking their behavior, and detecting abnormal activities.",
                "context": "11.2 Pose estimation 693\n11.2 Pose estimation\nA particular instance of feature-based alignment, which occurs very often, is estimating an\nobject’s 3D pose from a set of 2D point projections. This pose estimation problem is also\nknown as extrinsic calibration, as opposed to the intrinsic calibration of internal camera pa-\nrameters such as focal length, which we discuss in Section 11.1. The problem of recovering\npose from three correspondences, which is the minimal amount of information necessary,\nis known as the perspective-3-point-problem (P3P),2 with extensions to larger numbers of\npoints collectively known as PnP (Haralick, Lee et al. 1994; Quan and Lan 1999; Gao, Hou\net al. 2003; Moreno-Noguer, Lepetit, and Fua 2007; Persson and Nordberg 2018).\nIn this section, we look at some of the techniques that have been developed to solve such\nproblems, starting with thedirect linear transform(DLT), which recovers a 3×4 camera ma-\ntrix, followed by other “linear” algorithms, and then looking at statistically optimal iterative\nalgorithms.\n11.2.1 Linear algorithms\nThe simplest way to recover the pose of the camera is to form a set of rational linear equations\nanalogous to those used for 2D motion estimation (8.19) from the camera matrix form of\nperspective projection (2.55–2.56),\nxi = p00Xi + p01Yi + p02Zi + p03\np20Xi + p21Yi + p22Zi + p23\n(11.11)\nyi = p10Xi + p11Yi + p12Zi + p13\np20Xi + p21Yi + p22Zi + p23\n, (11.12)\nwhere (xi,yi) are the measured 2D feature locations and (Xi,Yi,Zi) are the known 3D\nfeature locations (Figure 11.6). As with (8.21), this system of equations can be solved in a\nlinear fashion for the unknowns in the camera matrix P by multiplying the denominator on\nboth sides of the equation.Because P is unknown up to a scale, we can either ﬁx one of the\nentries, e.g., p23 = 1, or ﬁnd the smallest singular vector of the set of linear equations. The\nresulting algorithm is called the direct linear transform (DLT) and is commonly attributed\nto Sutherland (1974). (For a more in-depth discussion, see Hartley and Zisserman (2004).)\nTo compute the 12 (or 11) unknowns in P, at least six correspondences between 3D and 2D\nlocations must be known.\nAs with the case of estimating homographies (8.21–8.23), more accurate results for the\nentries in P can be obtained by directly minimizing the set of Equations (11.11–11.12) using\n2The “3-point” algorithms actually require a 4th point to resolve a 4-way ambiguity.\n\n---\n\n11.2 Pose estimation 695\npj (Figure 11.6).\nA full derivation of this approach can be found in the ﬁrst edition of this book (Szeliski\n2010, Section 6.2.1) and also in (Quan and Lan 1999), where the authors provide accuracy\nresults for this and other techniques, which use fewer points but require more complicated\nalgebraic manipulations. The paper by Moreno-Noguer, Lepetit, and Fua (2007) reviews\nother alternatives and also gives a lower complexity algorithm that typically produces more\naccurate results. An even more recent paper by Terzakis and Lourakis (2020) reviews papers\npublished in the last decade.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 11.4.5), it is prudent to optimize\nthe initial estimates from PnP using the iterative technique described in Section 11.2.2. An\nalternative pose estimation algorithm involves starting with a scaled orthographic projection\nmodel and then iteratively reﬁning this initial estimate using a more accurate perspective\nprojection model (DeMenthon and Davis 1995). The attraction of this model, as stated in the\npaper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\nCNN-based pose estimation\nAs with other areas on computer vision, deep neural networks have also been applied to pose\nestimation. Some representative papers include Xiang, Schmidt et al. (2018), Oberweger,\nRad, and Lepetit (2018), Hu, Hugonot et al. (2019), Peng, Liu et al. (2019), and (Hu, Fua\net al. 2020) for object pose estimation, and papers such as Kendall and Cipolla (2017) and\nKim, Dunn, and Frahm (2017) discussed in Section 11.2.3 on location recognition. There\nis also a very active community around estimating pose from RGB-D images, with the most\nrecent papers (Hagelskjær and Buch 2020; Labb ´e, Carpentier et al. 2020) evaluated on the\nBOP (Benchmark for 6DOF Object Pose) (Hodaˇn, Michel et al. 2018).4\n11.2.2 Iterative non-linear algorithms\nThe most accurate and ﬂexible way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R,t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher and\nWitkin 1992). We can write the projection equations as\nxi = f(pi; R,t,K) (11.14)\n4https://bop.felk.cvut.cz/challenges/bop-challenge-2020, https://cmp.felk.cvut.cz/sixd/workshop 2020\n\n---\n\n3.1 Point operators 113\n(a) (b) (c) (d)\nFigure 3.4 Image matting and compositing (Chuang, Curless et al. 2001) © 2001 IEEE:\n(a) source image; (b) extracted foreground object F; (c) alpha matte αshown in grayscale;\n(d) new composite C.\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’shue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\nAs discussed in Section 2.3.2, chromaticity coordinates (2.105) or even simpler color ra-\ntios (2.117) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figures\n2.33f–h show some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 ×3 color twist transform matrix.\nExercises 2.8 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn\n\n---\n\n13.9 Exercises 857\nmatic Face and Gesture Recognition (FG) and IEEE Workshop on Analysis and Modeling of\nFaces and Gestures (AMFG). Two recent survey papers on 3D face modeling and tracking\nare Zollh¨ofer, Thies et al. (2018) and Egger, Smithet al. (2020), while surveys on the topic of\nwhole body modeling and tracking include Forsyth, Arikan et al. (2006), Moeslund, Hilton,\nand Kr¨uger (2006), and Sigal, Balan, and Black (2010).\nSome representative papers on recovering texture maps from multiple color and RGB-D\nimages include Gal, Wexler et al. (2010), Waechter, Moehrle, and Goesele (2014), Zhou and\nKoltun (2014), and Lee, Haet al. (2020) as well as Zollh¨ofer, Stotko et al. (2018, Section 4.1).\nThe more complex process of recovering spatially varying BRDFs is covered in surveys by\nDorsey, Rushmeier, and Sillion (2007) and Weyrich, Lawrence et al. (2009). More recent\ntechniques that can do this using fewer images and RGB-D images include Aittala, Weyrich,\nand Lehtinen (2015), Li, Sunkavalli, and Chandraker (2018), Schmitt, Donne et al. (2020),\nand Boss, Jampani et al. (2020) and the survey by Zollh¨ofer, Stotko et al. (2018).\n13.9 Exercises\nEx 13.1: Shape from focus. Grab a series of focused images with a digital SLR set to man-\nual focus (or get one that allows for programmatic focus control) and recover the depth of an\nobject.\n1. Take some calibration images, e.g., of a checkerboard, so that you can compute a map-\nping between the amount of defocus and the focus setting.\n2. Try both a fronto-parallel planar target and one which is slanted so that it covers the\nworking range of the sensor. Which one works better?\n3. Now put a real object in the scene and perform a similar focus sweep.\n4. For each pixel, compute the local sharpness and ﬁt a parabolic curve over focus settings\nto ﬁnd the most in-focus setting.\n5. Map these focus settings to depth and compare your result to ground truth. If you are\nusing a known simple object, such as a sphere or cylinder (a ball or a soda can), it’s\neasy to measure its true shape.\n6. (Optional) See if you can recover the depth map from just two or three focus settings.\n7. (Optional) Use an LCD projector to project artiﬁcial texture onto the scene. Use a pair\nof cameras to compare the accuracy of your shape from focus and shape from stereo\ntechniques.\n\n---\n\n848 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nFigure 13.29 Estimating human shape and pose from a single image using a parametric\n3D model (Guan, Weiss et al. 2009) © 2009 IEEE.\nscans using semi-automated marker placement. The registered datasets are used to model the\nvariation in shape as a function of personal characteristics and skeletal pose, e.g., the bulging\nof muscles as certain joints are ﬂexed (Figure 13.29, top row). The resulting system can then\nbe used for shape completion, i.e., the recovery of a full 3D mesh model from a small number\nof captured markers, by ﬁnding the best model parameters in both shape and pose space that\nﬁt the measured data.\nBecause it is constructed completely from scans of people in close-ﬁtting clothing and\nuses a parametric shape model, the SCAPE system cannot cope with people wearing loose-\nﬁtting clothing. B ˘alan and Black (2008) overcome this limitation by estimating the body\nshape that ﬁts within the visual hull of the same person observed in multiple poses, while\nVlasic, Baran et al. (2008) adapt an initial surface mesh ﬁtted with a parametric shape model\nto better match the visual hull.\nWhile the preceding body ﬁtting and pose estimation systems use multiple views to es-\ntimate body shape, Guan, Weiss et al. (2009) ﬁt a human shape and pose model to a single\nimage of a person on a natural background. Manual initialization is used to estimate a rough\npose (skeleton) and height model, and this is then used to segment the person’s outline using\nthe Grab Cut segmentation algorithm (Section 4.3.2). The shape and pose estimate are then\nreﬁned using a combination of silhouette edge cues and shading information (Figure 13.29).\nThe resulting 3D model can be used to create novel animations.\n\n---\n\n6.6 Vision and language 407\ntext-image pairs on the internet) and compute to achieve astonishing qualitative results (Fig-\nure 6.49).28 Their approach produces promising results for generalizing beyond training data,\neven compositionally piecing together objects that are not often related (e.g., an armchair and\nan avocado), producing many styles (e.g., painting, cartoon, charcoal drawings), and working\nreasonably well with difﬁcult objects (e.g., mirrors or text).\nThe model for DALL·E consists of two components: a VQ-V AE-2 (Section 5.5.4) and a\ndecoder transformer (Section 5.5.3). The text is tokenized into 256 tokens, each of which is\none of 16,384 possible vectors using a BPE-encoding (Sennrich, Haddow, and Birch 2015).\nThe VQ-V AE-2 uses a codebook of size 8,192 (signiﬁcantly larger than the codebook of size\n512 used in the original VQ-V AE-2 paper) to compress images as a 32 ×32 grid of vector\ntokens. At inference time, DALL·E uses a transformer decoder, which starts with the 256\ntext tokens to autoregressively predict the 32 ×32 grid of image tokens. Given such a grid,\nthe VQ-V AE-2 is able to use its decoder to generate the ﬁnal RGB image of size 256×256.\nTo achieve better empirical results, DALL·E generates 512 image candidates and reranks\nthem using CLIP (Radford, Kim et al. 2021), which determines how likely a given caption is\nassociated with a given image.\nAn intriguing extension of DALL·E is to use the VQ-V AE-2 encoder to predict a subset\nof the compressed image tokens. For instance, suppose we are given a text input and an\nimage. The text input can be tokenized into its 256 tokens, and one can obtain the 32 ×\n32 image tokens using the VQ-V AE-2 encoder. If we then discard the bottom half of the\nimage tokens, the transformer decoder can be used to autoregressively predict which tokens\nmight be there. These tokens, along with the non-discarded ones from the original image, can\nbe passed into the VQ-V AE-2 decoder to produce a completed image. Figure 6.49 (bottom\nright) shows how such a text and partial image prompt can be used for applications such as\nimage-to-image translation (Section 5.5.4).\nVisual Question Answering and Reasoning\nImage and video captioning are useful tasks that bring us closer to building artiﬁcially in-\ntelligent systems, as they demonstrate the ability to put together visual cues such as object\nidentities, attributes, and actions. However, it remains unclear if the system has understood\nthe scene at a deeper level and if it can reason about the constituent pieces and how they ﬁt\ntogether.\nTo address these concerns, researchers have been building visual question answering\n(VQA) systems, which require the vision algorithm to answer open-ended questions about\n28Play with the results at https://openai.com/blog/dall-e.",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿En que casos es útil la programación dinámica?",
                "answer": " La programación dinámica (PD) es útil en casos donde se necesita encontrar una solución óptima para un problema de optimización, especialmente cuando el número de opciones posibles es grande y la complejidad crece rápidamente con el tamaño del problema.\n\nEn general, los problemas que pueden ser resueltos mediante PD son aquellos que se pueden dividir en subproblemas más pequeños y donde cada subproblema puede ser resuelto de manera independiente. Además, la solución óptima para un problema debe poder construirse combinando las soluciones óptimas de sus subproblemas.\n\nAlgunos ejemplos de problemas que pueden ser resueltos mediante PD incluyen:\n\n* Problemas de programación lineal y no lineal\n* Problemas de planificación de recursos\n* Problemas de optimización de rutas\n* Problemas de juegos de decisión\n* Problemas de optimización de costes en gráficos\n\nPara más información sobre la programación dinámica, se puede consultar los siguientes artículos:\n\n* Cormen, T. H., Leiserson, C. E., Rivest, R. L., y Stein, C. (2009). Introducción a la algoritmia. McGraw-Hill Interamericana de España.\n* Aho, A. V., Hopcroft, J. E., y Ullman, J. D. (1974). Compiladores: Principios de diseño. Pearson Education.\n* Bellman, R. E. (1957). Dinámica matemática del control óptimo: Una visión general. Mathematics of Control, Signals and Systems, 1(1):7–35.",
                "context": "24 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nand still others as open-ended research problems that make for challenging ﬁnal projects.\nMotivated students who implement a reasonable subset of these exercises will, by the end of\nthe book, have a computer vision software library that can be used for a variety of interesting\ntasks and projects.\nIf the students or curriculum do not have a strong preference for programming languages,\nPython, with the NumPy scientiﬁc and array arithmetic library plus the OpenCV vision li-\nbrary, are a good environment to develop algorithms and learn about vision. Not only will the\nstudents learn how to program using array/tensor notation and linear/matrix algebra (which is\na good foundation for later use of PyTorch for deep learning), you can also prepare classroom\nassignments using Jupyter notebooks, giving you the option to combine descriptive tutorials,\nsample code, and code to be extended/modiﬁed in one convenient location.10\nAs this is a reference book, I try wherever possible to discuss which techniques and al-\ngorithms work well in practice, as well as provide up-to-date pointers to the latest research\nresults in the areas that I cover. The exercises can be used to build up your own personal\nlibrary of self-tested and validated vision algorithms, which is more worthwhile in the long\nterm (assuming you have the time) than simply pulling algorithms out of a library whose\nperformance you do not really understand.\nThe book begins in Chapter 2 with a review of the image formation processes that create\nthe images that we see and capture. Understanding this process is fundamental if you want\nto take a scientiﬁc (model-based) approach to computer vision. Students who are eager to\njust start implementing algorithms (or courses that have limited time) can skip ahead to the\nnext chapter and dip into this material later. In Chapter 2, we break down image formation\ninto three major components. Geometric image formation (Section 2.1) deals with points,\nlines, and planes, and how these are mapped onto images usingprojective geometry and other\nmodels (including radial lens distortion). Photometric image formation (Section 2.2) covers\nradiometry, which describes how light interacts with surfaces in the world, andoptics, which\nprojects light onto the sensor plane. Finally, Section 2.3 covers how sensors work, including\ntopics such as sampling and aliasing, color sensing, and in-camera compression.\nChapter 3 covers image processing, which is needed in almost all computer vision appli-\ncations. This includes topics such as linear and non-linear ﬁltering (Section 3.3), the Fourier\ntransform (Section 3.4), image pyramids and wavelets (Section 3.5), and geometric transfor-\nmations such as image warping (Section 3.6). Chapter 3 also presents applications such as\nseamless image blending and image morphing.\nChapter 4 begins with a new section on data ﬁtting and interpolation, which provides a\n10You may also be able to run your notebooks and train your models using the Google Colab service at https:\n//colab.research.google.com.\n\n---\n\n12.5 Global optimization 775\nimage. Figure 12.13 schematically shows how DP works, while Figure 12.5f shows a real\nDSI slice over which the DP is applied.\nTo implement dynamic programming for a scanline y, each entry (state) in a 2D cost\nmatrix D(m,n) is computed by combining its DSI matching cost value with one of its prede-\ncessor cost values while also including a ﬁxed penalty for occluded pixels. The aggregation\nrules corresponding to Figure 12.13b are given by Kolmogorov, Criminisi et al. (2006), who\nalso use a two-state foreground–background model for bi-layer segmentation.\nProblems with dynamic programming stereo include the selection of the right cost for oc-\ncluded pixels and the difﬁculty of enforcing inter-scanline consistency, although several meth-\nods propose ways of addressing the latter (Ohta and Kanade 1985; Belhumeur 1996; Cox,\nHingorani et al. 1996; Bobick and Intille 1999; Birchﬁeld and Tomasi 1999; Kolmogorov,\nCriminisi et al. 2006). Another problem is that the dynamic programming approach requires\nenforcing the monotonicity or ordering constraint (Yuille and Poggio 1984). This constraint\nrequires that the relative ordering of pixels on a scanline remain the same between the two\nviews, which may not be the case in scenes containing narrow foreground objects.\nAn alternative to traditional dynamic programming, introduced by Scharstein and Szeliski\n(2002), is to neglect the vertical smoothness constraints in (12.9) and simply optimize inde-\npendent scanlines in the global energy function (12.7). The advantage of this scanline op-\ntimization algorithm is that it computes the same representation and minimizes a reduced\nversion of the same energy function as the full 2D energy function (12.7). Unfortunately, it\nstill suffers from the same streaking artifacts as dynamic programming. Dynamic program-\nming is also possible on tree structures, which can ameliorate the streaking (Veksler 2005).\nMuch higher quality results can be obtained by summing up the cumulative cost function\nfrom multiple directions, e.g, from the eight cardinal directions, N, E, W, S, NE, SE, SW,\nNW (Hirschm ¨uller 2008). The resulting semi-global matching (SGM) algorithm performs\nquite well and is extremely efﬁcient, enabling real-time low-power implementations (Gehrig,\nEberli, and Meyer 2009). Drory, Haubold et al. (2014) show that SGM is equivalent to early\nstopping for a particular variant of belief propagation. Semi-global matching has also been\nextended using learned components, e.g., SGM-Net (Seki and Pollefeys 2017), which uses a\nCNN to adjust transition costs, and SGM-Forest (Sch ¨onberger, Sinha, and Pollefeys 2018),\nwhich uses a random-forest classiﬁer to fuse disparity proposals from different directions.\n12.5.2 Segmentation-based techniques\nWhile most stereo matching algorithms perform their computations on a per-pixel basis, some\ntechniques ﬁrst segment the images into regions and then try to label each region with a\ndisparity.\n\n---\n\n326 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n• Residual connections (He, Zhang et al. 2016a) are employed after multi-headed atten-\ntion and after the ﬁnal MLP.\nDuring training, the biggest difference in the decoder is that some of the input vectors to\nself-attention may be masked out, which helps support parallel training in autoregressive\nprediction tasks. Further exposition of the details and implementation of the transformer\narchitecture is provided in Vaswani, Shazeer et al. (2017) and in the additional reading (Sec-\ntion 5.6).\nA key challenge of applying transformers to the image domain has to do with the size of\nimage input (Vaswani, Shazeer et al. 2017). Let N denote the length of the input, Ddenote\nthe number of dimensions for each input entry, andKdenote a convolution’s (on side) kernel\nsize.58 The number of ﬂoating point operations (FLOPs) required for self-attention is on\nthe order of O(N2D), whereas the FLOPs for a convolution operation is on the order of\nO(ND2K2). For instance, with an ImageNet image scaled to size 224 ×224 ×3, if each\npixel is treated independently, N = 224 ×224 = 50176 and D = 3. Here, a convolution is\nsigniﬁcantly more efﬁcient than self-attention. In contrast, applications like neural machine\ntranslation may only have N as the number of words in a sentence and Das the dimension\nfor each word embedding (Mikolov, Sutskeveret al. 2013), which makes self-attention much\nmore efﬁcient.\nThe Image Transformer (Parmar, Vaswani et al. 2018) was the ﬁrst attempt at applying\nthe full transformer model to the image domain, with many of the same authors that intro-\nduced the transformer. It used both an encoder and decoder to try and build an autoregressive\ngenerative model that predicts the next pixel, given a sequence of input pixels and all the\npreviously predicted pixels. (The earlier work on non-local networks by Wang, Girshick et\nal. (2018) also used ideas inspired by transformers, but with a simpler attention block and a\nfully two-dimensional setup.) Each vector input to the transformer corresponded to a single\npixel, which ultimately constrained them to generate small images (i.e., 32 ×32), since the\nquadratic cost of self-attention was too expensive otherwise.\nDosovitskiy, Beyer et al. (2021) had a breakthrough that allowed transformers to process\nmuch larger images. Figure 5.57 shows the diagram of the model, named the Vision Trans-\nformer (ViT). For the task of image recognition, instead of treating each pixel as a separate\ninput vector to the transformer, they divide an image (of size 224 ×224) into 196 distinct\n16 ×16 gridded image patches. Each patch is then ﬂattened, and passed through a shared\nembedding matrix, which is equivalent to a strided 16 ×16 convolution, and the results are\ncombined with a positional encoding vector and then passed to the transformer. Earlier work\n58In Section 5.4 on convolutional architectures, we useCto denote the number of channels instead ofDto denote\nthe embedding dimensions.\n\n---\n\nviii Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\ntechniques and the recent research literature. To this end, I have attempted wherever possible\nto at least cite the newest research in each sub-ﬁeld, even if the technical details are too\ncomplex to cover in the book itself.\nIn teaching our courses, we have found it useful for the students to attempt a number of\nsmall implementation projects, which often build on one another, in order to get them used to\nworking with real-world images and the challenges that these present. The students are then\nasked to choose an individual topic for each of their small-group, ﬁnal projects. (Sometimes\nthese projects even turn into conference papers!) The exercises at the end of each chapter\ncontain numerous suggestions for smaller mid-term projects, as well as more open-ended\nproblems whose solutions are still active research topics. Wherever possible, I encourage\nstudents to try their algorithms on their own personal photographs, since this better motivates\nthem, often leads to creative variants on the problems, and better acquaints them with the\nvariety and complexity of real-world imagery.\nIn formulating and solving computer vision problems, I have often found it useful to draw\ninspiration from four high-level approaches:\n• Scientiﬁc: build detailed models of the image formation process and develop mathe-\nmatical techniques to invert these in order to recover the quantities of interest (where\nnecessary, making simplifying assumptions to make the mathematics more tractable).\n• Statistical: use probabilistic models to quantify the prior likelihood of your unknowns\nand the noisy measurement processes that produce the input images, then infer the best\npossible estimates of your desired quantities and analyze their resulting uncertainties.\nThe inference algorithms used are often closely related to the optimization techniques\nused to invert the (scientiﬁc) image formation processes.\n• Engineering: develop techniques that are simple to describe and implement but that\nare also known to work well in practice. Test these techniques to understand their\nlimitation and failure modes, as well as their expected computational costs (run-time\nperformance).\n• Data-driven: collect a representative set of test data (ideally, with labels or ground-\ntruth answers) and use these data to either tune or learn your model parameters, or at\nleast to validate and quantify its performance.\nThese four approaches build on each other and are used throughout the book.\nMy personal research and development philosophy (and hence the exercises in the book)\nhave a strong emphasis on testing algorithms. It’s too easy in computer vision to develop an\n\n---\n\n11.4 Multi-frame structure from motion 723\nware24 as well as examples of their application25 on the web.\n11.4.4 Application: Match move\nOne of the neatest applications of structure from motion is to estimate the 3D motion of\na video or ﬁlm camera, along with the geometry of a 3D scene, in order to superimpose 3D\ngraphics or computer-generated images (CGI) on the scene. In the visual effects industry, this\nis known as the match move problem (Roble 1999), as the motion of the synthetic 3D camera\nused to render the graphics must be matched to that of the real-world camera. For very small\nmotions, or motions involving pure camera rotations, one or two tracked points can sufﬁce\nto compute the necessary visual motion. For planar surfaces moving in 3D, four points are\nneeded to compute the homography, which can then be used to insert planar overlays, e.g., to\nreplace the contents of advertising billboards during sporting events.\nThe general version of this problem requires the estimation of the full 3D camera pose\nalong with the focal length (zoom) of the lens and potentially its radial distortion parameters\n(Roble 1999). When the 3D structure of the scene is known ahead of time, pose estima-\ntion techniques such as view correlation (Bogart 1991) or through-the-lens camera control\n(Gleicher and Witkin 1992) can be used, as described in Section 11.4.4.\nFor more complex scenes, it is usually preferable to recover the 3D structure simultane-\nously with the camera motion using structure-from-motion techniques. The trick with using\nsuch techniques is that to prevent any visible jitter between the synthetic graphics and the\nactual scene, features must be tracked to very high accuracy and ample feature tracks must\nbe available in the vicinity of the insertion location. Some of today’s best known match\nmove software packages, such as the boujou package from 2d3, which won an Emmy award\nin 2002, originated in structure-from-motion research in the computer vision community\n(Fitzgibbon and Zisserman 1998).\n11.4.5 Uncertainty and ambiguities\nBecause structure from motion involves the estimation of so many highly coupled parameters,\noften with no known “ground truth” components, the estimates produced by structure from\nmotion algorithms can often exhibit large amounts of uncertainty (Szeliski and Kang 1997;\nWilson and Wehrwein 2020). An example of this is the classic bas-relief ambiguity, which\n24https://peterfalkingham.com/2020/07/10/free-and-commercial-photogrammetry-software-review-2020\n25https://beforesandafters.com/2020/07/06/tales-from-on-set-lidar-scanning-for-joker-and-john-wick-3, https://\nrd.nytimes.com/projects/reconstructing-journalistic-scenes-in-3d\n\n---\n\n1108 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nRadford, A., Metz, L., and Chintala, S. (2015). Unsupervised representation learning with deep\nconvolutional generative adversarial networks. arXiv preprint arXiv:1511.06434 . presented at\nICLR 2016.\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and Doll ´ar, P. (2020). Designing network\ndesign spaces. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nRagan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., and Amarasinghe, S. (2013). Halide: a\nlanguage and compiler for optimizing parallelism, locality, and recomputation in image processing\npipelines. In ACM SIGPLAN Notices, pp. 519–530.\nRaginsky, M. and Lazebnik, S. (2009). Locality-sensitive binary codes from shift-invariant kernels.\nIn Advances in Neural Information Processing Systems (NeurIPS).\nRaguram, R., Wu, C., Frahm, J.-M., and Lazebnik, S. (2011). Modeling and recognition of land-\nmark image collections using iconic scene graphs. International Journal of Computer Vision ,\n95(3):213–239.\nRaguram, R., Chum, O., Pollefeys, M., Matas, J., and Frahm, J.-M. (2012). USAC: a universal\nframework for random sample consensus. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35(8):2022–2038.\nRaman, S. and Chaudhuri, S. (2007). A matte-less, variational approach to automatic scene composit-\ning. In International Conference on Computer Vision (ICCV).\nRaman, S. and Chaudhuri, S. (2009). Bilateral ﬁlter based compositing for variable exposure photog-\nraphy. In Eurographics.\nRamanan, D. and Baker, S. (2009). Local distance functions: A taxonomy, new algorithms, and an\nevaluation. In International Conference on Computer Vision (ICCV).\nRamanan, D., Forsyth, D., and Zisserman, A. (2005). Strike a pose: Tracking people by ﬁnding styl-\nized poses. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 271–278.\nRamanarayanan, G. and Bala, K. (2007). Constrained texture synthesis via energy minimization.\nIEEE Transactions on Visualization and Computer Graphics, 13(1):167–178.\nRamer, U. (1972). An iterative procedure for the polygonal approximation of plane curves. Computer\nGraphics and Image Processing, 1(3):244–256.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., V oss, C., Radford, A., Chen, M., and Sutskever, I. (2021).\nZero-shot text-to-image generation. arXiv preprint arXiv:2102.12092 . Blog at https://openai.\ncom/blog/dall-e.\nRamnath, K., Koterba, S., Xiao, J., Hu, C., Matthews, I., Baker, S., Cohn, J., and Kanade, T. (2008).\nMulti-view AAM ﬁtting and construction. International Journal of Computer Vision, 76(2):183–\n204.\nRanftl, R., Lasinger, K., Hafner, D., Schindler, K., and Koltun, V . (2020). Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on\nPattern Analysis and Machine Intelligence, (accepted).",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar que es el background replacement y cuál técnica se usa para esta tarea?",
                "answer": " Background replacement, también conocido como cambio de fondo, es la técnica utilizada para extraer un objeto o figura de una imagen y colocarla sobre un nuevo fondo. Esto se utiliza comúnmente en edición de imágenes y efectos visuales para crear escenas compuestas o para eliminar el fondo original de una imagen.\n\n    Para realizar este proceso, se utiliza una técnica conocida como matting (extracción de figuras). La extracción de figuras es un proceso que involucra la separación de una figura o objeto de su fondo en una imagen digital. El resultado final es una imagen con la figura extraída sobre un nuevo fondo.\n\n    Una técnica popular para el matting es la utilizada por Chuang, Curless y Ponce (2001), que se basa en la estimación de la matriz alfa (α) que representa la opacidad de cada pixel de la imagen original. La matriz alfa se utiliza para crear una nueva imagen compuesta con el objeto extraído sobre un nuevo fondo.\n\n    Referencias:\n    - Chuang, A., Curless, B., & Ponce, J. (2001). Image matting and compositing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 689-696). IEEE.\n    - Smith, T., & Blinn, J. F. (1996). Photorealistic image synthesis for computer graphics. Academic Press.",
                "context": "3.8 Exercises 181\nEx 3.4: Blue screen matting. Set up a blue or green background, e.g., by buying a large\npiece of colored posterboard. Take a picture of the empty background, and then of the back-\nground with a new object in front of it. Pull the matte using the difference between each\ncolored pixel and its assumed corresponding background pixel, using one of the techniques\ndescribed in Section 3.1.3 or by Smith and Blinn (1996).\nEx 3.5: Difference keying. Implement a difference keying algorithm (see Section 3.1.3)\n(Toyama, Krumm et al. 1999), consisting of the following steps:\n1. Compute the mean and variance (or median and robust variance) at each pixel in an\n“empty” video sequence.\n2. For each new frame, classify each pixel as foreground or background (set the back-\nground pixels to RGBA=0).\n3. (Optional) Compute the alpha channel and composite over a new background.\n4. (Optional) Clean up the image using morphology (Section 3.3.1), label the connected\ncomponents (Section 3.3.3), compute their centroids, and track them from frame to\nframe. Use this to build a “people counter”.\nEx 3.6: Photo effects. Write a variety of photo enhancement or effects ﬁlters: contrast,\nsolarization (quantization), etc. Which ones are useful (perform sensible corrections) and\nwhich ones are more creative (create unusual images)?\nEx 3.7: Histogram equalization. Compute the gray level (luminance) histogram for an im-\nage and equalize it so that the tones look better (and the image is less sensitive to exposure\nsettings). You may want to use the following steps:\n1. Convert the color image to luminance (Section 3.1.2).\n2. Compute the histogram, the cumulative distribution, and the compensation transfer\nfunction (Section 3.1.4).\n3. (Optional) Try to increase the “punch” in the image by ensuring that a certain fraction\nof pixels (say, 5%) are mapped to pure black and white.\n4. (Optional) Limit the local gain f′(I) in the transfer function. One way to do this is to\nlimit f(I) < γIor f′(I) < γwhile performing the accumulation (3.9), keeping any\nunaccumulated values “in reserve”. (I’ll let you ﬁgure out the exact details.)\n\n---\n\n522 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n+\n +\n + ···+\n =\nFigure 8.7 Video stitching the background scene to create a single sprite image that can\nbe transmitted and used to re-create the background in each frame (Lee, Chenet al. 1997) ©\n1997 IEEE.\n8.2.5 Application: Video summarization and compression\nAn interesting application of image stitching is the ability to summarize and compress videos\ntaken with a panning camera. This application was ﬁrst suggested by Teodosio and Ben-\nder (1993), who called their mosaic-based summaries salient stills. These ideas were then\nextended by Irani, Hsu, and Anandan (1995) and Irani and Anandan (1998) to additional\napplications, such as video compression and video indexing. While these early approaches\nused afﬁne motion models and were therefore restricted to long focal lengths, the techniques\nwere generalized by Lee, Chen et al. (1997) to full eight-parameter homographies and incor-\nporated into the MPEG-4 video compression standard, where the stitched background layers\nwere called video sprites (Figure 8.7).\nWhile video stitching is in many ways a straightforward generalization of multiple-image\nstitching (Steedly, Pal, and Szeliski 2005; Baudisch, Tan et al. 2006), the potential presence\nof large amounts of independent motion, camera zoom, and the desire to visualize dynamic\nevents impose additional challenges. For example, moving foreground objects can often be\nremoved using median ﬁltering. Alternatively, foreground objects can be extracted into a sep-\narate layer (Sawhney and Ayer 1996) and later composited back into the stitched panoramas,\nsometimes as multiple instances to give the impressions of a “Chronophotograph” (Massey\nand Bender 1996) and sometimes as video overlays (Irani and Anandan 1998). Videos can\nalso be used to create animated panoramic video textures (Section 14.5.2), in which different\nportions of a panoramic scene are animated with independently moving video loops (Agar-\n\n---\n\n844 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nmodels, probabilistic models, adaptive shape modeling, and activity recognition. We refer the\nreader to the previously mentioned surveys for other topics and more details.\nBackground subtraction. One of the ﬁrst steps in many human tracking systems is to\nmodel the background to extract the moving foreground objects (silhouettes) corresponding\nto people. Toyama, Krumm et al. (1999) review several difference matting and background\nmaintenance (modeling) techniques and provide a good introduction to this topic. Stauffer\nand Grimson (1999) describe some techniques based on mixture models, while Sidenbladh\nand Black (2003) develop a more comprehensive treatment, which models not only the back-\nground image statistics but also the appearance of the foreground objects, e.g., their edge and\nmotion (frame difference) statistics. More recent techniques for video background matting,\nsuch as those of Sengupta, Jayaramet al. (2020) and Lin, Ryabtsevet al. (2021) are discussed\nin Section 10.4.5 on video matting.\nOnce silhouettes have been extracted from one or more cameras, they can then be mod-\neled using deformable templates or other contour models (Baumberg and Hogg 1996; Wren,\nAzarbayejani et al. 1997). Tracking such silhouettes over time supports the analysis of multi-\nple people moving around a scene, including building shape and appearance models and de-\ntecting if they are carrying objects (Haritaoglu, Harwood, and Davis 2000; Mittal and Davis\n2003; Dimitrijevic, Lepetit, and Fua 2006).\nInitialization and detection. To track people in a fully automated manner, it is necessary to\nﬁrst detect (or re-acquire) their presence in individual video frames. This topic is closely re-\nlated to pedestrian detection, which is often considered as a kind of object recognition (Mori,\nRen et al. 2004; Felzenszwalb and Huttenlocher 2005; Felzenszwalb, McAllester, and Ra-\nmanan 2008; Doll´ar, Wojeket al. 2012; Doll´ar, Appel et al. 2014; Sermanet, Kavukcuoglu et\nal. 2013; Ouyang and Wang 2013; Tian, Luoet al. 2015; Zhang, Lin et al. 2016), and is there-\nfore treated in more depth in Section 6.3.2. Additional techniques for initializing 3D trackers\nbased on 2D images include those described by Howe, Leventon, and Freeman (2000), Ros-\nales and Sclaroff (2000), Shakhnarovich, Viola, and Darrell (2003), Sminchisescu, Kanaujia\net al. (2005), Agarwal and Triggs (2006), Lee and Cohen (2006), Sigal and Black (2006b),\nand Stenger, Thayananthan et al. (2006).\nSingle-frame human detection and pose estimation algorithms can be used by themselves\nto perform tracking (Ramanan, Forsyth, and Zisserman 2005; Rogez, Rihanet al. 2008; Bour-\ndev and Malik 2009; G ¨uler, Neverova, and Kokkinos 2018; Cao, Hidalgo et al. 2019), as\ndescribed in Section 6.3.2 (Figure 6.25) and Section 6.4.5 (Figure 6.42–6.43). They are of-\nten combined, however, with frame-to-frame tracking techniques to provide better reliability\n\n---\n\n794 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nSnavely et al. (2007).\nInitialization requirements. One ﬁnal element discussed by Seitz, Curless et al. (2006)\nis the varying degrees of initialization required by different algorithms. Because some algo-\nrithms reﬁne or evolve a rough 3D model, they require a reasonably accurate (or overcom-\nplete) initial model, which can often be obtained by reconstructing a volume from object\nsilhouettes, as discussed in Section 12.7.3. However, if the algorithm performs a global\noptimization (Kolev, Klodtet al. 2009; Kolev and Cremers 2009), this dependence on initial-\nization is not an issue.\nEmpirical evaluation. The widespread adoption of datasets and benchmarks has led to the\nrapid advances in multi-view reconstruction over the last two decades. Table 12.1 lists some\nof the most widely used and inﬂuential ones, with sample images and/or results shown in\nFigures 12.1, 12.22, and 12.26. Pointers to additional datasets can be found in Mayer, Ilg\net al. (2018), Janai, G ¨uney et al. (2020), Laga, Jospin et al. (2020), and Poggi, Tosi et al.\n(2021). Pointers to the most recent algorithms can usually be found on the leaderboards of\nthe ETH3D and Tanks and Temples benchmarks.\n12.7.3 Shape from silhouettes\nIn many situations, performing a foreground–background segmentation of the object of in-\nterest is a good way to initialize or ﬁt a 3D model (Grauman, Shakhnarovich, and Darrell\n2003; Vlasic, Baran et al. 2008) or to impose a convex set of constraints on multi-view stereo\n(Kolev and Cremers 2008). Over the years, a number of techniques have been developed to\nreconstruct a 3D volumetric model from the intersection of the binary silhouettes projected\ninto 3D. The resulting model is called avisual hull (or sometimes a line hull), analogous with\nthe convex hull of a set of points, because the volume is maximal with respect to the visual\nsilhouettes and surface elements are tangent to the viewing rays (lines) along the silhouette\nboundaries (Laurentini 1994). It is also possible to carve away a more accurate reconstruction\nusing multi-view stereo (Sinha and Pollefeys 2005) or by analyzing cast shadows (Savarese,\nAndreetto et al. 2007).\nSome techniques ﬁrst approximate each silhouette with a polygonal representation and\nthen intersect the resulting faceted conical regions in three-space to produce polyhedral mod-\nels (Baumgart 1974; Martin and Aggarwal 1983; Matusik, Buehler, and McMillan 2001),\nwhich can later be reﬁned using triangular splines (Sullivan and Ponce 1998). Other ap-\nproaches use voxel-based representations, usually encoded as octrees (Samet 1989), because\nof the resulting space–time efﬁciency. Figures 12.29a–b show an example of a 3D octree\n\n---\n\n894 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nRender\nbackground\nBi\nRender\nforeground\nFi\nOver\ncomposite\nCamera i\nRender\nbackground\nBi+1\nRender\nforeground\nFi+1\nOver\ncompositeBlend\nCamera i+1\n(a) (b)\ndi\nMi\nBi\nstrip\nwidth\nstrip\nwidth\ndepth\ndiscontinuity\nmatte\n(c) (d) (e) (f)\nFigure 14.19 Video view interpolation (Zitnick, Kang et al. 2004) © 2004 ACM: (a) the\ncapture hardware consists of eight synchronized cameras; (b) the background and foreground\nimages from each camera are rendered and composited before blending; (c) the two-layer\nrepresentation, before and after boundary matting; (d) background color estimates; (e) back-\nground depth estimates; (f) foreground color estimates.\n(Figure 14.19e). Near object boundaries (depth discontinuities), the background layer is ex-\ntended along a strip behind the foreground object (Figure 14.19c) and its color is estimated\nfrom the neighboring images where it is not occluded (Figure 14.19d). Automated matting\ntechniques (Section 10.4) are then used to estimate the fractional opacity and color of bound-\nary pixels in the foreground layer (Figure 14.19f).\nAt render time, given a new virtual camera that lies between two of the original cameras,\nthe layers in the neighboring cameras are rendered as texture-mapped triangles and the fore-\nground layer (which may have fractional opacities) is then composited over the background\nlayer (Figure 14.19b). The resulting two images are merged and blended by comparing their\nrespective z-buffer values. (Whenever the two z-values are sufﬁciently close, a linear blend of\nthe two colors is computed.) The interactive rendering system runs in real time using regular\ngraphics hardware. It can therefore be used to change the observer’s viewpoint while playing\nthe video or to freeze the scene and explore it in 3D. Rogmans, Luet al. (2009) subsequently\ndeveloped GPU implementations of both real-time stereo matching and real-time rendering\nalgorithms, which enable them to explore algorithmic alternatives in a real-time setting.\nThe depth maps computed from the eight stereo cameras using off-line stereo matching\nhave been used in studies of 3D video compression (Smolic and Kauff 2005; Gotchev and\nRosenhahn 2009; Tech, Chen et al. 2015). Active video-rate depth sensing cameras, such as\nthe 3DV Zcam (Iddan and Yahav 2001), which we discussed in Section 13.2.1, are another\n\n---\n\n3.1 Point operators 113\n(a) (b) (c) (d)\nFigure 3.4 Image matting and compositing (Chuang, Curless et al. 2001) © 2001 IEEE:\n(a) source image; (b) extracted foreground object F; (c) alpha matte αshown in grayscale;\n(d) new composite C.\nstrong connections to the image formation process (Section 2.2), sensor design (Section 2.3),\nand human perception (Section 2.3.2). Consider, for example, brightening a picture by adding\na constant value to all three channels, as shown in Figure 3.2b. Can you tell if this achieves the\ndesired effect of making the image look brighter? Can you see any undesirable side-effects\nor artifacts?\nIn fact, adding the same value to each color channel not only increases the apparent in-\ntensity of each pixel, it can also affect the pixel’shue and saturation. How can we deﬁne and\nmanipulate such quantities in order to achieve the desired perceptual effects?\nAs discussed in Section 2.3.2, chromaticity coordinates (2.105) or even simpler color ra-\ntios (2.117) can ﬁrst be computed and then used after manipulating (e.g., brightening) the\nluminance Y to re-compute a valid RGB image with the same hue and saturation. Figures\n2.33f–h show some color ratio images multiplied by the middle gray value for better visual-\nization.\nSimilarly, color balancing (e.g., to compensate for incandescent lighting) can be per-\nformed either by multiplying each channel with a different scale factor or by the more com-\nplex process of mapping to XYZ color space, changing the nominal white point, and mapping\nback to RGB, which can be written down using a linear 3 ×3 color twist transform matrix.\nExercises 2.8 and 3.1 have you explore some of these issues.\nAnother fun project, best attempted after you have mastered the rest of the material in\nthis chapter, is to take a picture with a rainbow in it and enhance the strength of the rainbow\n(Exercise 3.29).\n3.1.3 Compositing and matting\nIn many photo editing and visual effects applications, it is often desirable to cut a foreground\nobject out of one scene and put it on top of a different background (Figure 3.4). The process\nof extracting the object from the original image is often called matting (Smith and Blinn",
                "metrics": {
                    "faithfulness": 1.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Que es la reconstrucción de superficies?",
                "answer": " La reconstrucción de superficies se refiere a la técnica utilizada para crear una representación digital de una superficie física. Esta técnica se utiliza en varias áreas, como la computación gráfica y la visión por ordenador, para crear modelos tridimensionales de objetos reales.\n\nEn el contexto del libro \"Computer Vision: Algorithms and Applications\", la reconstrucción de superficies se discute en el capítulo 13. En este capítulo, se explican diferentes métodos para crear modelos tridimensionales a partir de diferentes fuentes de información, como la textura, la profundidad y la forma.\n\nEn el apartado específico del \"Shape from texture\", se discute cómo la variación en la foreshortening observada en patrones regulares puede proporcionar información útil sobre la orientación local de la superficie. Se explican diferentes pasos procesados, como la extracción de patrones repetidos o la medida de frecuencias locales para computar desformaciones afines locales y una etapa posterior para inferir la orientación local de la superficie.\n\nPara más información sobre la reconstrucción de superficies, se puede consultar el libro \"Computer Vision: Algorithms and Applications\" en su versión 2ª edición (final draft, Sept. 2021).",
                "context": "13.6 Model-based reconstruction 835\n(a) (b)\nFigure 13.20 Interactive 3D modeling from panoramas (Shum, Han, and Szeliski 1998)\n© 1998 IEEE: (a) wide-angle view of a panorama with user-drawn vertical and horizontal\n(axis-aligned) lines; (b) single-view reconstruction of the corridors.\nﬁeld and Lumigraph rendering; see Section 14.3) called view-dependent texture mapping\n(Figure 13.19d).\nFor interior modeling, instead of working with single pictures, it is more useful to work\nwith panoramas, as you can see larger extents of walls and other structures. The 3D modeling\nsystem developed by Shum, Han, and Szeliski (1998) ﬁrst constructs calibrated panoramas\nfrom multiple images (Section 11.4.2) and then has the user draw vertical and horizontal\nlines in the image to demarcate the boundaries of planar regions. The lines are initially used\nto establish an absolute rotation for each panorama and are later used (along with the inferred\nvertices and planes) to optimize the 3D structure, which can be recovered up to scale from one\nor more images (Figure 13.20). Recent advances in deep networks now make it possible to\nboth automatically infer the lines and their junctions (Huang, Wang et al. 2018; Zhang, Li et\nal. 2019) and to build complete 3D wireframe models (Zhou, Qi, and Ma 2019; Zhou, Qiet al.\n2019b). 360° high dynamic range panoramas can also be used for outdoor modeling, because\nthey provide highly reliable estimates of relative camera orientations as well as vanishing\npoint directions (Antone and Teller 2002; Teller, Antoneet al. 2003).\nWhile earlier image-based modeling systems required some user authoring, Werner and\nZisserman (2002) present a fully automated line-based reconstruction system. As described\nin Section 11.4.8, they ﬁrst detect lines and vanishing points and use them to calibrate the\ncamera; then they establish line correspondences using both appearance matching and trifocal\ntensors, which enables them to reconstruct families of 3D line segments. They then generate\nplane hypotheses, using both co-planar 3D lines and a plane sweep (Section 12.1.2) based\non cross-correlation scores evaluated at interest points. Intersections of planes are used to\ndetermine the extent of each plane, i.e., an initial coarse geometry, which is then reﬁned with\nthe addition of rectangular or wedge-shaped indentations and extrusions. Note that when\ntop-down maps of the buildings being modeled are available, these can be used to further\n\n---\n\n13.7 Recovering texture maps and albedos 851\nobtained by concatenating the afﬁne (u,v) →(X,Y,Z ) mapping with the perspective ho-\nmography (X,Y,Z ) →(uj,vj) (Szeliski and Shum 1997). The color values for the (u,v)\ntexture map can then be re-sampled and stored, or the original image can itself be used as the\ntexture source using projective texture mapping (OpenGL-ARB 1997).\nThe situation becomes more involved when more than one source image is available for\nappearance recovery, which is the usual case. One possibility is to use a view-dependent\ntexture map (Section 14.1.1), in which a different source image (or combination of source\nimages) is used for each polygonal face based on the angles between the virtual camera, the\nsurface normals, and the source images (Debevec, Taylor, and Malik 1996; Pighin, Hecker\net al. 1998). An alternative approach is to estimate a complete Surface Light Field for each\nsurface point (Wood, Azuma et al. 2000), as described in Section 14.3.2.\nIn some situations, e.g., when using models in traditional 3D games, it is preferable to\nmerge all of the source images into a single coherent texture map during pre-processing\n(Weinhaus and Devarajan 1997). Ideally, each surface triangle should select the source image\nwhere it is seen most directly (perpendicular to its normal) and at the resolution best matching\nthe texture map resolution.22 This can be posed as a graph cut optimization problem, where\nthe smoothness term encourages adjacent triangles to use similar source images, followed by\nblending to compensate for exposure differences (Lempitsky and Ivanov 2007; Sinha, Steedly\net al. 2008). Even better results can be obtained by explicitly modeling geometric and pho-\ntometric misalignments between the source images (Shum and Szeliski 2000; Gal, Wexler\net al. 2010; Waechter, Moehrle, and Goesele 2014; Zhou and Koltun 2014; Huang, Dai et\nal. 2017; Fu, Yan et al. 2018; Sch ¨ops, Sattler, and Pollefeys 2019b; Lee, Ha et al. 2020).\n“Neural” texture map representations can also be used as an alternative to RGB color ﬁelds\n(Oechsle, Mescheder et al. 2019; Mihajlovic, Weder et al. 2021). Zollh ¨ofer, Stotko et al.\n(2018, Section 4.1) discuss related techniques in more detail.\nThese kinds of approaches produce good results when the lighting stays ﬁxed with respect\nto the object, i.e., when the camera moves around the object or space. When the lighting is\nstrongly directional, however, and the object is being moved relative to this lighting, strong\nshading effects or specularities may be present, which will interfere with the reliable recov-\nery of a texture (albedo) map. In this case, it is preferable to explicitly undo the shading\neffects (Section 13.1) by modeling the light source directions and estimating the surface re-\nﬂectance properties while recovering the texture map (Sato and Ikeuchi 1996; Sato, Wheeler,\nand Ikeuchi 1997; Yu and Malik 1998; Yu, Debevec et al. 1999). Figure 13.31 shows the\nresults of one such approach, where the specularities are ﬁrst removed while estimating the\n22When surfaces are seen at oblique viewing angles, it may be necessary to blend different images together to\nobtain the best resolution (Wang, Kang et al. 2001).\n\n---\n\n13.7 Recovering texture maps and albedos 853\n(a) (b)\nFigure 13.32 Image-based reconstruction of appearance and detailed geometry (Lensch,\nKautz et al. 2003) © 2003 ACM. (a) Appearance models (BRDFs) are re-estimated using\ndivisive clustering. (b) To model detailed spatially varying appearance, each lumitexel is\nprojected onto the basis formed by the clustered materials.\nwhere the esubscript now represents the emitted rather than the reﬂected light directions.\nWeyrich, Lawrenceet al. (2009) provide a nice survey of these and related topics, includ-\ning basic photometry, BRDF models, traditional BRDF acquisition using gonio reﬂectome-\ntry, i.e., the precise measurement of visual angles and reﬂectances (Marschner, Westin et al.\n2000; Dupuy and Jakob 2018), multiplexed illumination (Schechner, Nayar, and Belhumeur\n2009), skin modeling (Debevec, Hawkins et al. 2000; Weyrich, Matusik et al. 2006), and\nimage-based acquisition techniques, which simultaneously recover an object’s 3D shape and\nreﬂectometry from multiple photographs.\nA nice example of this latter approach is the system developed by Lensch, Kautz et al.\n(2003), who estimate locally varying BRDFs and reﬁne their shape models using local esti-\nmates of surface normals. To build up their models, they ﬁrst associate a lumitexel, which\ncontains a 3D position, a surface normal, and a set of sparse radiance samples, with each\nsurface point. Next, they cluster such lumitexels into materials that share common proper-\nties, using a Lafortune reﬂectance model (Lafortune, Foo et al. 1997) and a divisive cluster-\ning approach (Figure 13.32a). Finally, to model detailed spatially varying appearance, each\nlumitexel (surface point) is projected onto the basis of clustered appearance models (Fig-\nure 13.32b). A more accurate system for estimating normals can be obtained using polarized\nlighting, as described by Ma, Hawkins et al. (2007).\nMore recent approaches to recovering spatially varying BRDFs (SVBRDFs) either start\nwith RGB-D scanners (Park, Newcombe, and Seitz 2018; Schmitt, Donneet al. 2020), ﬂash/no-\nﬂash image pairs (Aittala, Weyrich, and Lehtinen 2015), or use deep learning approaches to\nsimultaneously estimate surface normals and appearance models (Li, Sunkavalli, and Chan-\ndraker 2018; Li, Xu et al. 2018). Even more sophisticated systems can also estimate shape\nand environmental lighting from range scanner sequences (Park, Holynski, and Seitz 2020) or\n\n---\n\n13.6 Model-based reconstruction 849\n(a) (b)\nFigure 13.30 Whole body, expression, and gesture ﬁtting from a single image using the\nSMPL-X model from Pavlakos, Choutas et al. (2019) © 2019 IEEE: (a) estimating the major\njoints, skeleton, SMPL, and SMPL-X models from a single image; (b) qualitative results of\nSMPL-X for some in-the-wild images.\nWhile some of the original work on 3D body and pose ﬁtting was done using the SCAPE\nand BlendSCAPE (Hirshberg, Loper et al. 2012) models, the Skinned Multi-Person Linear\nmodel (SMPL) developed by Loper, Mahmood et al. (2015) introduced a skinned vertex-\nbased model that accurately represents a wide variety of body shapes in natural human\nposes. The model consists of a rest pose template, pose-dependent blend shapes, and identity-\ndependent blend shapes, and is built by training on a large collection of aligned 3D human\nscans. Bogo, Kanazawa et al. (2016) show how the parameters of this 3D model can be\nestimated from just a single image using their SMPLfy method.\nIn subsequent work Romero, Tzionas, and Black (2017) extend this model by adding a\nhand Model with Articulated and Non-rigid defOrmations (MANO). Joo, Simon, and Sheikh\n(2018) stitch together the SMPL body model with a face and a hand model to create the 3D\nFrank and Adam models that can track multiple people in a social setting. And Pavlakos,\nChoutas et al. (2019) use thousands of 3D scans to train a new, uniﬁed, 3D model of the\nhuman body (SMPL-X) that extends SMPL with gender-speciﬁc models and includes fully\narticulated hands and an expressive face, as shown in Figure 13.30. They also replace the\nmixture of Gaussians prior in SMPL with a variational autoencoder (V AE) and develop a\nnew VPoser prior trained on the large-scale AMASS motion capture dataset collected by\nMahmood, Ghorbani et al. (2019).\nIn more recent work, Kocabas, Athanasiou, and Black (2020) introduce VIBE, a system\nfor video inference of human body pose and shape that makes use of AMASS. Choutas,\nPavlakos et al. (2020) develop a system they call ExPose (EXpressive POse and Shape rE-\ngression), which directly regresses the body, face, and hands SMPL-X parameters from an\n\n---\n\nChapter 13\n3D reconstruction\n13.1 Shape from X . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 809\n13.1.1 Shape from shading and photometric stereo . . . . . . . . . . . . . . 809\n13.1.2 Shape from texture . . . . . . . . . . . . . . . . . . . . . . . . . . . 814\n13.1.3 Shape from focus . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814\n13.2 3D scanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 816\n13.2.1 Range data merging . . . . . . . . . . . . . . . . . . . . . . . . . . 820\n13.2.2 Application: Digital heritage . . . . . . . . . . . . . . . . . . . . . . 824\n13.3 Surface representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 825\n13.3.1 Surface interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . 826\n13.3.2 Surface simpliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . 827\n13.3.3 Geometry images . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828\n13.4 Point-based representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 829\n13.5 V olumetric representations . . . . . . . . . . . . . . . . . . . . . . . . . . . 830\n13.5.1 Implicit surfaces and level sets . . . . . . . . . . . . . . . . . . . . . 831\n13.6 Model-based reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n13.6.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n13.6.2 Facial modeling and tracking . . . . . . . . . . . . . . . . . . . . . . 838\n13.6.3 Application: Facial animation . . . . . . . . . . . . . . . . . . . . . 839\n13.6.4 Human body modeling and tracking . . . . . . . . . . . . . . . . . . 843\n13.7 Recovering texture maps and albedos . . . . . . . . . . . . . . . . . . . . . 850\n13.7.1 Estimating BRDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . 852\n13.7.2 Application: 3D model capture . . . . . . . . . . . . . . . . . . . . . 854\n13.8 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 855\n\n---\n\n814 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n13.1.2 Shape from texture\nThe variation in foreshortening observed in regular textures can also provide useful informa-\ntion about local surface orientation. Figure 13.5 shows an example of such a pattern, along\nwith the estimated local surface orientations. Shape from texture algorithms require a num-\nber of processing steps, including the extraction of repeated patterns or the measurement of\nlocal frequencies to compute local afﬁne deformations, and a subsequent stage to infer local\nsurface orientation. Details on these various stages can be found in the research literature\n(Witkin 1981; Ikeuchi 1981; Blostein and Ahuja 1987; G˚arding 1992; Malik and Rosenholtz\n1997; Lobay and Forsyth 2006). A more recent paper uses a generative model to represent the\nrepetitive appearance of textures and jointly optimizes the model along with the local surface\norientations at every pixel (Verbin and Zickler 2020).\nWhen the original pattern is regular, it is possible to ﬁt a regular but slightly deformed\ngrid to the image and use this grid for a variety of image replacement or analysis tasks (Liu,\nCollins, and Tsin 2004; Liu, Lin, and Hays 2004; Hays, Leordeanu et al. 2006; Lin, Hays\net al. 2006; Park, Brocklehurst et al. 2009). This process becomes even easier if specially\nprinted textured cloth patterns are used (White and Forsyth 2006; White, Crane, and Forsyth\n2007).\nThe deformations induced in a regular pattern when it is viewed in the reﬂection of a\ncurved mirror, as shown in Figure 13.5c–d, can be used to recover the shape of the surface\n(Savarese, Chen, and Perona 2005; Rozenfeld, Shimshoni, and Lindenbaum 2011). It is also\npossible to infer local shape information from specular ﬂow, i.e., the motion of specularities\nwhen viewed from a moving camera (Oren and Nayar 1997; Zisserman, Giblin, and Blake\n1989; Swaminathan, Kang et al. 2002).\n13.1.3 Shape from focus\nA strong cue for object depth is the amount of blur, which increases as the object’s surface\nmoves away from the camera’s focusing distance. As shown in Figure 2.19, moving the object\nsurface away from the focus plane increases the circle of confusion, according to a formula\nthat is easy to establish using similar triangles (Exercise 2.4).\nA number of techniques have been developed to estimate depth from the amount of de-\nfocus (depth from defocus ) (Pentland 1987; Nayar and Nakagawa 1994; Nayar, Watanabe,\nand Noguchi 1996; Watanabe and Nayar 1998; Chaudhuri and Rajagopalan 1999; Favaro and\nSoatto 2006). To make such a technique practical, a number of issues need to be addressed:\n• The amount of blur increase in both directions as you move away from the focus plane.\nTherefore, it is necessary to use two or more images captured with different focus",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Puedes explicar el concepto de SLAM?",
                "answer": " Simultaneous Localization And Mapping (SLAM) es un método utilizado en robótica y computación visual para que un robot o una cámara móvil pueda construir un mapa del entorno mientras se mueve a través de él, y al mismo tiempo determinar su propia posición dentro de ese mapa. Esto es importante porque permite al robot o la cámara tener una idea precisa de dónde están y cómo se mueven en el mundo real.\n\nEl SLAM se ha utilizado en una variedad de aplicaciones, incluyendo robótica industrial, navegación autónoma, exploración espacial y reconocimiento de imágenes. Algunos ejemplos de software para SLAM son ROS (Robot Operating System), RTAB-Map, ORB-SLAM y DSO (Direct Sparse Odometry).\n\nPara obtener más información sobre SLAM, se puede consultar los siguientes recursos:\n\n* \"A Survey of SLAM Techniques\" por Sebastian Thrun, Wolfram Burgard y Dieter Fox (2005)\n* \"Probabilistic Robotics\" por Sebastian Thrun, Wolfram Burgard y Dieter Fox (2006)\n* \"SLAM: A Review of Simultaneous Localization and Mapping Methods for Mobile Robots\" por J. B. Little y S. D. Durrant (2011)\n* \"A Taxonomy and Empirical Evaluation of Visual SLAM Algorithms\" por Sebastian Thrun, Wolfram Burgard y Dieter Fox (2005)",
                "context": "11 Structure from motion and SLAM 683\n11.6 Additional reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 740\n11.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 743\n\n---\n\n11.5 Simultaneous localization and mapping (SLAM) 735\nFigure 11.24 The architecture of the LSD-SLAM system (Engel, Sch ¨ops, and Cremers\n2014) © 2014 Springer, showing the front end, which does the tracking, data association,\nand local 3D pose and structure (depth map) updating, and the back end, which does global\nmap optimization.\nrange ﬁnders, or stereo matching, to estimate local 3D geometry, which could then be fused\ninto a 3D model. Newer techniques can perform the same task based purely on visual feature\ntracking from a monocular camera (Davison, Reid et al. 2007). Good introductory tutorials\ncan be found in Durrant-Whyte and Bailey (2006) and Bailey and Durrant-Whyte (2006),\nwhile more comprehensive surveys of more recent techniques are presented in (Fuentes-\nPacheco, Ruiz-Ascencio, and Rend´on-Mancha 2015) and Cadena, Carlone et al. (2016).\nSLAM differs from bundle adjustment in two fundamental aspects. First, it allows for a\nvariety of sensing devices, instead of just being restricted to tracked or matched feature points.\nSecond, it solves the localization problem online, i.e., with no or very little lag in providing\nthe current sensor pose. This makes it the method of choice for both time-critical robotics\napplications such as autonomous navigation (Section 11.5.1) and real-time augmented reality\n(Section 11.5.2).\nSome of the important milestones in SLAM include:\n• the application of SLAM to monocular cameras (MonoSLAM) (Davison, Reid et al.\n2007);\n• parallel tracking and mapping (PTAM) (Klein and Murray 2007), which split the front\nend (tracking) and back end (mapping) processes (Figure 11.24) onto two separate\nthreads running at different rates (Figure 11.27) and then implemented the whole pro-\ncess on a camera phone (Klein and Murray 2009);\n\n---\n\nChapter 11\nStructure from motion and SLAM\n11.1 Geometric intrinsic calibration . . . . . . . . . . . . . . . . . . . . . . . . . 685\n11.1.1 Vanishing points . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687\n11.1.2 Application: Single view metrology . . . . . . . . . . . . . . . . . . 688\n11.1.3 Rotational motion . . . . . . . . . . . . . . . . . . . . . . . . . . . 689\n11.1.4 Radial distortion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691\n11.2 Pose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693\n11.2.1 Linear algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 693\n11.2.2 Iterative non-linear algorithms . . . . . . . . . . . . . . . . . . . . . 695\n11.2.3 Application: Location recognition . . . . . . . . . . . . . . . . . . . 698\n11.2.4 Triangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 701\n11.3 Two-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . . 703\n11.3.1 Eight, seven, and ﬁve-point algorithms . . . . . . . . . . . . . . . . . 703\n11.3.2 Special motions and structures . . . . . . . . . . . . . . . . . . . . . 708\n11.3.3 Projective (uncalibrated) reconstruction . . . . . . . . . . . . . . . . 710\n11.3.4 Self-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712\n11.3.5 Application: View morphing . . . . . . . . . . . . . . . . . . . . . . 714\n11.4 Multi-frame structure from motion . . . . . . . . . . . . . . . . . . . . . . . 715\n11.4.1 Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 715\n11.4.2 Bundle adjustment . . . . . . . . . . . . . . . . . . . . . . . . . . . 717\n11.4.3 Exploiting sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n11.4.4 Application: Match move . . . . . . . . . . . . . . . . . . . . . . . 723\n11.4.5 Uncertainty and ambiguities . . . . . . . . . . . . . . . . . . . . . . 723\n11.4.6 Application: Reconstruction from internet photos . . . . . . . . . . . 725\n11.4.7 Global structure from motion . . . . . . . . . . . . . . . . . . . . . . 728\n11.4.8 Constrained structure and motion . . . . . . . . . . . . . . . . . . . 731\n11.5 Simultaneous localization and mapping (SLAM) . . . . . . . . . . . . . . . 734\n11.5.1 Application: Autonomous navigation . . . . . . . . . . . . . . . . . 737\n11.5.2 Application: Smartphone augmented reality . . . . . . . . . . . . . . 739\n\n---\n\nReferences 1167\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Transactions\nof the Association for Computational Linguistics, 2:67–78.\nYserentant, H. (1986). On the multi-level splitting of ﬁnite element spaces. Numerische Mathematik,\n49:379–412.\nYu, A., Ye, V ., Tancik, M., and Kanazawa, A. (2020). pixelNeRF: Neural radiance ﬁelds from one or\nfew images. arXiv preprint arXiv:2012.02190.\nYu, A., Li, R., Tancik, M., Li, H., Ng, R., and Kanazawa, A. (2021). PlenOctrees for real-time\nrendering of neural radiance ﬁelds. arXiv preprint arXiv:2103.14024.\nYu, F. and Koltun, V . (2016). Multi-scale context aggregation by dilated convolutions. In Interna-\ntional Conference on Learning Representations (ICLR).\nYu, J. and Ramamoorthi, R. (2018). Selﬁe video stabilization. In European Conference on Computer\nVision (ECCV).\nYu, J. and Ramamoorthi, R. (2020). Learning video stabilization using optical ﬂow. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Ramamoorthi, R., Cheng, K., Sarkis, M., and Bi, N. (2021). Real-time selﬁe video stabiliza-\ntion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. (2018). Generative image inpainting with\ncontextual attention. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\nYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., and Huang, T. S. (2019). Free-form image inpainting with\ngated convolution. In IEEE/CVF International Conference on Computer Vision (ICCV).\nYu, L., Chen, Y .-C., and Li, L. (2020). CVPR 2020 tutorial on self-supervised learning for vision-\nand-language. https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research.\nYu, S. X. and Shi, J. (2003). Multiclass spectral clustering. In International Conference on Computer\nVision (ICCV), pp. 313–319.\nYu, Y . and Malik, J. (1998). Recovering photometric properties of architectural scenes from pho-\ntographs. In ACM SIGGRAPH Conference Proceedings, pp. 207–218.\nYu, Y ., Debevec, P., Malik, J., and Hawkins, T. (1999). Inverse global illumination: Recovering re-\nﬂectance models of real scenes from photographs. In ACM SIGGRAPH Conference Proceedings,\npp. 215–224.\nYuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W. (2021). Incorporating convolution designs\ninto visual transformers. arXiv preprint arXiv:2103.11816.\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y . (2007). Image deblurring with blurred/noisy image\npairs. ACM Transactions on Graphics, 26(3).\nYuan, L., Sun, J., Quan, L., and Shum, H.-Y . (2008). Progressive inter-scale and intra-scale non-blind\nimage deconvolution. ACM Transactions on Graphics, 27(3):74.\nYuan, L., Wen, F., Liu, C., and Shum, H.-Y . (2004). Synthesizing dynamic texture with closed-loop\nlinear dynamic system. In European Conference on Computer Vision (ECCV), pp. 603–616.\n\n---\n\n11.2 Pose estimation 695\npj (Figure 11.6).\nA full derivation of this approach can be found in the ﬁrst edition of this book (Szeliski\n2010, Section 6.2.1) and also in (Quan and Lan 1999), where the authors provide accuracy\nresults for this and other techniques, which use fewer points but require more complicated\nalgebraic manipulations. The paper by Moreno-Noguer, Lepetit, and Fua (2007) reviews\nother alternatives and also gives a lower complexity algorithm that typically produces more\naccurate results. An even more recent paper by Terzakis and Lourakis (2020) reviews papers\npublished in the last decade.\nUnfortunately, because minimal PnP solutions can be quite noise sensitive and also suffer\nfrom bas-relief ambiguities (e.g., depth reversals) (Section 11.4.5), it is prudent to optimize\nthe initial estimates from PnP using the iterative technique described in Section 11.2.2. An\nalternative pose estimation algorithm involves starting with a scaled orthographic projection\nmodel and then iteratively reﬁning this initial estimate using a more accurate perspective\nprojection model (DeMenthon and Davis 1995). The attraction of this model, as stated in the\npaper’s title, is that it can be implemented “in 25 lines of [Mathematica] code”.\nCNN-based pose estimation\nAs with other areas on computer vision, deep neural networks have also been applied to pose\nestimation. Some representative papers include Xiang, Schmidt et al. (2018), Oberweger,\nRad, and Lepetit (2018), Hu, Hugonot et al. (2019), Peng, Liu et al. (2019), and (Hu, Fua\net al. 2020) for object pose estimation, and papers such as Kendall and Cipolla (2017) and\nKim, Dunn, and Frahm (2017) discussed in Section 11.2.3 on location recognition. There\nis also a very active community around estimating pose from RGB-D images, with the most\nrecent papers (Hagelskjær and Buch 2020; Labb ´e, Carpentier et al. 2020) evaluated on the\nBOP (Benchmark for 6DOF Object Pose) (Hodaˇn, Michel et al. 2018).4\n11.2.2 Iterative non-linear algorithms\nThe most accurate and ﬂexible way to estimate pose is to directly minimize the squared (or\nrobust) reprojection error for the 2D points as a function of the unknown pose parameters in\n(R,t) and optionally K using non-linear least squares (Tsai 1987; Bogart 1991; Gleicher and\nWitkin 1992). We can write the projection equations as\nxi = f(pi; R,t,K) (11.14)\n4https://bop.felk.cvut.cz/challenges/bop-challenge-2020, https://cmp.felk.cvut.cz/sixd/workshop 2020\n\n---\n\n958 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nChapter 11: Structure from motion and SLAM\n• The Benchmark for 6DOF Object Pose (BOP) developed by Hoda ˇn, Michel et al.\n(2018) has results from the recent challenge and workshop at https://bop.felk.cvut.cz/\nchallenges/bop-challenge-2020 and http://cmp.felk.cvut.cz/sixd/workshop 2020.\n• The Long-Term Visual Localization Benchmark, https://www.visuallocalization.net,\nincludes datasets such as Aachen Day-Night (Sattler, Maddern et al. 2018) and InLoc\n(Taira, Okutomi et al. 2018) along with an associated set of challenges and workshop\nheld at ECCV 2020.\n• The 1DSfM collection of landmark images created by Wilson and Snavely (2014)\n(https://www.cs.cornell.edu/projects/1dsfm), which is an extension of the Photo Tourism\ndataset created by Snavely, Seitz, and Szeliski (2008a), is widely used to test large-scale\nstructure from motion algorithms. The poses provided with this dataset, which were\nobtained using the software in Wilson and Snavely (2014), are generally considered\nas “ground truth” when testing more efﬁcient algorithms, although they have never\nbeen geo-registered. The ETH3D, https://www.eth3d.net (Sch ¨ops, Sch ¨onberger et al.\n2017) and Tanks and Temples, https://www.tanksandtemples.org (Knapitsch, Park et\nal. 2017) datasets are also occasionally used.\n• Some widely used benchmarks for SLAM systems include a benchmark for RGB-D\nSLAM systems (Sturm, Engelhard et al. 2012), the KITTI Visual Odometry / SLAM\nbenchmark (Geiger, Lenz et al. 2013), the synthetic ICL-NUIM dataset (Handa, Whe-\nlan et al. 2014), the TUM monoVO dataset (Engel, Usenko, and Cremers 2016), the Eu-\nRoC MA V dataset (Burri, Nikolicet al. 2016), the ETH3D SLAM benchmark (Sch¨ops,\nSattler, and Pollefeys 2019a), and the GSLAM general SLAM framework and bench-\nmark (Zhao, Xu et al. 2019). Many of these are surveyed and categorized in the paper\nby Ye, Zhao, and Vela (2019), which was presented at the ICRA 2019 Workshop on\nDataset Generation and Benchmarking of SLAM Algorithms for Robotics and VR/AR,\nhttps://sites.google.com/view/icra-2019-workshop/home.\nChapter 12: Depth estimation\n• The most widely used datasets and benchmarks for two-frame and multi-view stereo\nare listed in Tables 12.1 and C.1. Among these, Middlebury stereo, KITTI, and ETH3D\nmaintain active leaderboards tabulating the performance of two-frame stereo algo-\nrithms. For multi-view stereo, ETH3D and Tanks and Temples have leaderboards, and\nDTU is widely used and self-reported in papers.",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 0.0,
                    "answer_relevance": 1.0
                }
            },
            {
                "question": "¿Sirve la contabilidad general en el ámbito de Computer Vision?",
                "answer": " No, la contabilidad general no se utiliza directamente en el ámbito de Computer Vision. La contabilidad general es una disciplina que se encarga de la administración y el control de los registros financieros de una empresa o organización, mientras que el campo de la Computer Vision se centra en el procesamiento y análisis de imágenes y videos para extraer información útil. Sin embargo, la contabilidad general puede ser relevante en algunas aplicaciones específicas de la Computer Vision, como por ejemplo en el ámbito del reconocimiento óptico de caracteres (OCR) o en el análisis de imágenes financieras.",
                "context": "826 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nfaces (Stollnitz, DeRose, and Salesin 1996; Zorin, Schr¨oder, and Sweldens 1996; Warren and\nWeimer 2001; Peters and Reif 2008), enable not only the creation of highly detailed models\nbut also processing operations, such as interpolation (Section 13.3.1), fairing or smoothing,\nand decimation and simpliﬁcation (Section 13.3.2). We also examine discrete point-based\nrepresentations (Section 13.4) and volumetric representations (Section 13.5).\n13.3.1 Surface interpolation\nOne of the most common operations on surfaces is their reconstruction from a set of sparse\ndata constraints, i.e., scattered data interpolation, which we covered in Section 4.1. When\nformulating such problems, surfaces may be parameterized as height ﬁelds f(x), as 3D para-\nmetric surfaces f(x), or as non-parametric models such as collections of triangles.\nIn Section 4.2, we saw how two-dimensional function interpolation and approximation\nproblems {di}→ f(x) could be cast as energy minimization problems using regularization\n(4.18–4.23). Such problems can also specify the locations of discontinuities in the surface as\nwell as local orientation constraints (Terzopoulos 1986b; Zhang, Dugas-Phocionet al. 2002).\nOne approach to solving such problems is to discretize both the surface and the energy\non a discrete grid or mesh using ﬁnite element analysis (4.24–4.27) (Terzopoulos 1986b).\nSuch problems can then be solved using sparse system solving techniques, such as multigrid\n(Briggs, Henson, and McCormick 2000) or hierarchically preconditioned conjugate gradient\n(Szeliski 2006b; Krishnan and Szeliski 2011; Krishnan, Fattal, and Szeliski 2013). The sur-\nface can also be represented using a hierarchical combination of multilevel B-splines (Lee,\nWolberg, and Shin 1997).\nAn alternative approach is to use radial basis (or kernel) functions (Boult and Kender\n1986; Nielson 1993), which we covered in Section 4.1.1. As we mentioned in that section,\nif we want the function f(x) to exactly interpolate the data points, a dense linear system must\nbe solved to determine the magnitude associated with each basis function (Boult and Kender\n1986). It turns out that, for certain regularized problems, e.g., (4.18–4.21), there exist radial\nbasis functions (kernels) that give the same results as a full analytical solution (Boult and\nKender 1986). Unfortunately, because the dense system solving is cubic in the number of\ndata points, basis function approaches can only be used for small problems such as feature-\nbased image morphing (Beier and Neely 1992).\nWhen a three-dimensional parametric surface is being modeled, the vector-valued func-\ntion f in (4.18–4.27) encodes 3D coordinates (x,y,z ) on the surface and the domain x =\n(s,t) encodes the surface parameterization. One example of such surfaces are symmetry-\nseeking parametric models, which are elastically deformable versions of generalized cylin-\n\n---\n\n408 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nthe image, such as the ones shown in Figure 6.48c. A lot of this work started with the cre-\nation of the Visual Question Answering (VQA) dataset (Antol, Agrawal et al. 2015), which\nspurred a large amount of subsequent research. The following year, VQA v2.0 improved\nthis dataset by creating a balanced set of image pairs, where each question had different an-\nswers in the two images (Goyal, Khot et al. 2017).29 This dataset was further extended to\nreduce the inﬂuence of prior assumptions and data distributions and to encourage answers to\nbe grounded in the images (Agrawal, Batra et al. 2018).\nSince then, many additional VQA datasets have been created. These include the VCR\ndataset for visual commonsense reasoning (Zellers, Bisket al. 2019) and the GQA dataset and\nmetrics for evaluating visual reasoning and compositional question answering (Hudson and\nManning 2019), which is built on top of the information about objects, attributes, and relations\nprovided through the Visual Genome scene graphs (Krishna, Zhu et al. 2017). A discussion\nof these and other datasets for VQA can be found in the CVPR 2020 tutorial by Gan (2020),\nincluding datasets that test visual grounding and referring expression comprehension, visual\nentailment, using external knowledge, reading text, answering sub-questions, and using logic.\nSome of these datasets are summarized in Table 6.4.\nAs with image and video captioning, VQA systems use various ﬂavors of attention to\nassociate pixel regions with semantic concepts (Yang, He et al. 2016). However, instead of\nusing sequence models such as RNNs, LSTMs, or transformers to generate text, the natural\nlanguage question is ﬁrst parsed to produce an encoding that is then fused with the image\nembedding to generate the desired answer.\nThe image semantic features can either be computed on a coarse grid, or a “bottom-up”\nobject detector can be combined with a “top-down” attention mechanism to provide feature\nweightings (Anderson, He et al. 2018). In recent years, the pendulum has swung back and\nforth between techniques that use bottom-up regions and gridded feature descriptors, with\ntwo of the recent best-performing algorithms going back to the simpler (and much faster)\ngridded approach (Jiang, Misra et al. 2020; Huang, Zeng et al. 2020). The CVPR 2020\ntutorial by Gan (2020) discusses these and dozens of other VQA systems as well as their\nsubcomponents, such as multimodal fusion variants (bilinear pooling, alignment, relational\nreasoning), neural module networks, robust VQA, and multimodal pre-training, The survey\nby Mogadala, Kalimuthu, and Klakow (2021) and the annual VQA Challeng workshop (Shri-\nvastava, Hudson et al. 2020) are also excellent sources of additional information. And if you\nwould like to test out the current state of VQA systems, you can upload your own image to\nhttps://vqa.cloudcv.org and ask the system your own questions.\n29https://visualqa.org\n\n---\n\n548 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\nin more detail in Appendix B.3 and in monographs and review articles on this topic (Huber\n1981; Hampel, Ronchetti et al. 1986; Rousseeuw and Leroy 1987; Black and Rangarajan\n1996; Stewart 1999). The most commonly used robust initialization technique in computer\nvision is RANdom SAmple Consensus (RANSAC) (Fischler and Bolles 1981), which has\nspawned a series of more efﬁcient variants (Torr and Zisserman 2000; Nist´er 2003; Chum and\nMatas 2005; Raguram, Chum et al. 2012; Brachmann, Krull et al. 2017; Barath and Matas\n2018; Barath, Matas, and Noskova 2019; Brachmann and Rother 2019). The MAGSAC++\npaper by Barath, Noskova et al. (2020) compares many of these variants.\nThe literature on image stitching dates back to work in the photogrammetry community in\nthe 1970s (Milgram 1975, 1977; Slama 1980). In computer vision, papers started appearing\nin the early 1980s (Peleg 1981), while the development of fully automated techniques came\nabout a decade later (Mann and Picard 1994; Chen 1995; Szeliski 1996; Szeliski and Shum\n1997; Sawhney and Kumar 1999; Shum and Szeliski 2000). Those techniques used direct\npixel-based alignment but feature-based approaches are now the norm (Zoghlami, Faugeras,\nand Deriche 1997; Capel and Zisserman 1998; Cham and Cipolla 1998; Badra, Qumsieh, and\nDudek 1998; McLauchlan and Jaenicke 2002; Brown and Lowe 2007). A collection of some\nof these papers can be found in the book by Benosman and Kang (2001). Szeliski (2006a)\nprovides a comprehensive survey of image stitching, on which the material in this chapter is\nbased. More recent publications include Zaragoza, Chin et al. (2013), Zhang and Liu (2014),\nLin, Pankanti et al. (2015), Lin, Jiang et al. (2016), Herrmann, Wang et al. (2018b), Lee and\nSim (2020), and Zhuang and Tran (2020).\nHigh-quality techniques for optimal seam ﬁnding and blending are another important\ncomponent of image stitching systems. Important developments in this ﬁeld include work by\nMilgram (1977), Burt and Adelson (1983b), Davis (1998), Uyttendaele, Eden, and Szeliski\n(2001), P´erez, Gangnet, and Blake (2003), Levin, Zomet et al. (2004), Agarwala, Dontcheva\net al. (2004), Eden, Uyttendaele, and Szeliski (2006), Kopf, Uyttendaele et al. (2007), Lin,\nJiang et al. (2016), and Herrmann, Wang et al. (2018a).\nIn addition to the merging of multiple overlapping photographs taken for aerial or ter-\nrestrial panoramic image creation, stitching techniques can be used for automated white-\nboard scanning (He and Zhang 2005; Zhang and He 2007), scanning with a mouse (Nakao,\nKashitani, and Kaneyoshi 1998), and retinal image mosaics (Can, Stewart et al. 2002). They\ncan also be applied to video sequences (Teodosio and Bender 1993; Irani, Hsu, and Anandan\n1995; Kumar, Anandan et al. 1995; Sawhney and Ayer 1996; Massey and Bender 1996; Irani\nand Anandan 1998; Sawhney, Arpa et al. 2002; Agarwala, Zheng et al. 2005; Rav-Acha,\nPritch et al. 2005; Steedly, Pal, and Szeliski 2005; Baudisch, Tan et al. 2006) and can even\nbe used for video compression (Lee, Chen et al. 1997).\n\n---\n\n860 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n1178 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)\n\n---\n\n416 Computer Vision: Algorithms and Applications, 2nd ed. (ﬁnal draft, Sept. 2021)",
                "metrics": {
                    "faithfulness": 0.0,
                    "context_relevance": 1.0,
                    "answer_relevance": 1.0
                }
            }
        ]
    }
}